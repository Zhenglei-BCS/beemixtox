---
title: "MDR simulation and Comparison of two curations"
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = F,
  warning = F
)
library(knitr)
library(kableExtra)
```


# Data Pre-processing

Please note that if you install the package, the data pre-processing is not needed. The curated dataset is included in the package. This section is to explicitly show the script used in the data selection and data curation step. 

## Query from EPA-Ecotox

- **Chemicals**
- **Effect Measurements **
  - Mortality Group
	    - Mortality
- **Endpoints **
  - Concentration Based Endpoints
	  - LD50
- **Species **
  - Name(s) / Number(s)
	  - honey bee
  - Kingdom: Animals
  - Name Search Type: Common Name
- **Test Conditions **
  - Test Locations
	  - Lab
- **Publication Options **

Note in this level reduced to 576 CAS

## Narrow down

- chemicals with repeated entries (i.e., more than 3 or 5 LD50 for the same chemical)
- relevant Guideline: 141-1
- 

## Difference between the EPA extraction and the Local DB extraction

- Local DB generated a much smaller dataset.
- Need test ID for further comparison.
- The cause could be either the zip file if not the files behind EPA UI (identified one duplicate in species names) or the transformation scheme in terms of units, other identifiers are not the same. 

```{r setup}
library(beemixtox)
library(tidyverse)
```

## Using the local DB extraction



```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE,eval=T)
```

```{r}
data(bees)
```


```{r eval=F}
tmp <- bees %>% filter(!is.na(conc1))

table(tmp$exposure_type,useNA = c("ifany")) %>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")

table(tmp$test_location,useNA = c("ifany")) %>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")

table(tmp$conc1_unit_conv,useNA = c("ifany")) %>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")

table(tmp$conc1_type) %>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")

table(tmp$duration_conv,useNA = c("ifany")) %>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")



table(tmp$test_type,useNA = c("ifany")) %>%  kable(., format = "markdown")
```


```{r eval=F}
table(paste("study duration:",tmp$duration_conv,tmp$duration_unit_conv,"; exposure duration:",tmp$exposure_duration_mean,tmp$exposure_duration_unit,"; exposure type:",tmp$exposure_type,"; species: ",tmp$common_name),useNA = c("ifany")) %>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")

# left_join(sps,ecotox_group_convert)

## save(results,file="~/Projects/beemixtox/data/bees.rda")
```


2.	Restrict the database for chemicals with repeated entries (i.e., more than 3 or 5 LD50 for the same chemical)

```{r eval=F}
bee1 <- bees %>% filter(!is.na(conc1)) %>% filter(test_type=="ACUTE")
bee1%>% group_by(chemical_name) %>% summarise(n=length(chemical_name))%>%  kable(., format = "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, fixed_thead = T, position = "left")
## Restrict the database for chemicals with repeated entries (i.e., more than 3 or 5 LD50 for the same chemical)
tmp <- bee1%>% group_by(chemical_name) %>% summarise(n=length(chemical_name))%>% filter(n>3)

bee1 <- bee1 %>% filter(chemical_name %in% tmp$chemical_name)
dim(bee1)
```

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=T)
```


# Correlations among variables

## check USEPA internal, not for external use

```{r}
## Analysis of Data.B (USEPA internal, not for external use)
Data.B.1 = Data.B[Data.B$GUIDELINE=="141-1",] #retrict to relevant guideline
Data.B.1 = droplevels(Data.B.1)
table(Data.B.1$TAXA)
table(Data.B.1$TGL)
Data.B.1$TOXICITY = as.numeric(as.character(Data.B.1$TOXICITY))
Data.B.1 = Data.B.1[complete.cases(Data.B.1$TOXICITY),]

#Only definitive values
Data.B.1 = Data.B.1[Data.B.1$TGL=="",]
B.Bee.var.N = aggregate(TOXICITY ~ CAS_NO, data =Data.B.1, length)
B.Bee.var.Ave = aggregate(TOXICITY ~ CAS_NO, data =Data.B.1, mean)
B.Bee.var.sd = aggregate(TOXICITY ~ CAS_NO, data =Data.B.1, sd)

# Merge
B.Bee.var = merge(B.Bee.var.N,B.Bee.var.Ave, by="CAS_NO")
B.Bee.var = merge(B.Bee.var,B.Bee.var.sd, by="CAS_NO")
names(B.Bee.var) [c(2:4)] = c("N", "Mean", "sd")

# Restrict to at least 5 cases per CAS
B.Bee.var = B.Bee.var[B.Bee.var$N>3,] # 16 chemicals

B.Bee.var$CV.Perc = 100*(B.Bee.var$Mean/B.Bee.var$sd)

# Summary of CV%
summary(B.Bee.var$CV.Perc)
plot(B.Bee.var$N,B.Bee.var$CV.Perc)
plot(log10(B.Bee.var$Mean), B.Bee.var$CV.Perc)
```

## check the curated dataset

```{r fig.width=8}
Data.2 <- Data.2 %>% mutate(logTox = log(Observed.Response.Mean))
mod = lm(log(Observed.Response.Mean) ~ CAS.Number + Conc.1.Type..Author. + Exposure.Type + Observed.Duration..Days., data = Data.2)
summary(mod)
car::Anova(mod) %>% pander::pander(.)

library(GGally)
ggpairs(Data.2[,c("logTox","Conc.1.Type..Author.","Exposure.Type","Observed.Duration..Days.")])
ggplot(data=Data.2,aes(x=Observed.Duration..Days.,y=log(Observed.Response.Mean)))+geom_point()

ldat <- Data.2 %>% dplyr::select(c(CAS.Number , Conc.1.Type..Author., Exposure.Type,logTox)) %>% pivot_longer(!logTox,names_to="variables",values_to="value") 
ggplot(ldat,aes(x=value,y=logTox))+geom_boxplot()+geom_point()+facet_wrap(~variables,scales = "free")+ theme(axis.text.x = element_text(angle = 90))
```

# Reproduce and Examine Belden's Approach

Synergy can be concluded if an MDR is greater than 1. However, the toxicity of the mixture could be due to toxicological interaction or simply variability (cv) during toxicity testing.





3.	Calculate the CV% of the LD50s per chemical, and compute summary statistics (median CV% and 95% CI for CV%) that are typical for these types of bee studies


```{r}
Data.2 %>%  summarise(mean_LD=mean(Observed.Response.Mean),sd_LD=sd(Observed.Response.Mean),cv=sd_LD/mean_LD,median_LD=median(Observed.Response.Mean))
library(skimr)
cv <- function(x){
  sd(x)/mean(x)
}
my_skim <- skim_with(numeric = sfl(cv))
Data.2 %>% select(c(CAS.Number , Conc.1.Type..Author., Exposure.Type,Observed.Response.Mean)) %>% my_skim()
```


```{r}
cv_sum <- Data.2 %>% group_by(CAS.Number,Chemical.Name) %>% summarise(mean_LD=mean(Observed.Response.Mean),sd_LD=sd(Observed.Response.Mean),cv=sd_LD/mean_LD)
cv_sum %>% ungroup() %>% my_skim()%>% yank("numeric") %>% select(-c( n_missing,complete_rate)) %>% kable(.,format="pipe",digits=2)

```


```{r}
Data.2 %>% group_by(CAS.Number,Chemical.Name) %>% my_skim(Observed.Response.Mean) %>% yank("numeric") %>% select(-c(skim_variable, n_missing,complete_rate)) %>% kable(.,format="pipe",digits=2)
```






# Simulation


Perform simulations as in Belden and Brain to determine the 95th of MCR values that reject the Null Hypothesis that the MCR values is greater than 1.

Values were generated for each toxicity test that would be conducted in a mixture study. For example, for a binary mixture AB, a value would be generated for compound A, compound B, and mixture AB.


- assumed a log-normal distribution based on specific CV value and randomly generated effective concentrations for theoretical active ingredients and a formulation mixture centered around a "true" value of 1 (10 000 iterations).
    - For example, for a binary mixture AB, a value would be generated for compound A, compound B, and mixture AB.  Each value was generated assuming a mean value 1.  
- For each iteration, an MDR was calculated. 
- The distribution of the resulting MDR values was then determined
- We repeated this process for CVs of 60%, 100%, and 140% (could do more).
- We also conducted the analysis for single, binary, and tertiary mixtures.


Note that the assumption is that we can use cv and given mean to calculate mean and variance, which could then tranform to the log-mean and log-sd. 

```{r}
mean_sim <- 1
cv <- c(0.6,1,1.4)
sd_sim <- cv*mean_sim
v_sim <- sd_sim^2
mu <- log(mean_sim/sqrt(1+v_sim/mean_sim^2))
sigma <- sqrt(log(1+v_sim/mean_sim^2))
```

```{r eval=F}
  MDRs <- sapply(1:Nsim,function(i){
    ECx <- rlnorm(nmix+1,meanlog = mu,sdlog=sigma)
    ECx_mix <- 1/(sum(p/ECx[1:nmix]))
    MDR <- ECx_mix/(ECx[1+nmix])
    MDR
  })
```


For each iteration, the resulting values for the active ingredients were used in *Concentration Addition Model* and by assuming a 1:1 mix the expected EC50 for the mixture was determined.

The idea would be: using the simulations that you performed based on the different CV% levels used as reference (i.e., 70%, 100%, 180%), what is the proportion of samples that would be classified wrongly as synergistic (when we know they are additive) if using the following reference levels of threshold MDR: 1.25, 2, 3, 5.

```{r}
cvs <- c(0.3,0.6,1,1.4,3)
mdrs <- c(1.25,2,3,5)
Nsim <- 10000
Nout <- 100
```

```{r}
res1 <- simCVn(cvs=cvs,mean_sim=1,p=1,Nsim=Nsim)

c1 <-res1%>%group_by(CV)%>% nest() %>% mutate(confusion=purrr::map(data,function(df) sapply(mdrs,function(x) sum(df>x)/Nsim)))%>% mutate(Mixture=1) %>% dplyr::select(-data) %>% unnest(cols=c(confusion)) %>% ungroup %>% mutate(cutoff=rep(mdrs,length(cvs))) %>% pivot_wider(names_from = cutoff,names_prefix="MDR>",values_from=confusion)

r1 <- res1%>%group_by(CV)%>% summarise(mean=mean(MDR),q95=quantile(MDR,0.95)) %>% mutate(Mixture=1)


ggplot(res1, aes(MDR,col=CV)) + stat_ecdf(geom = "point")+xlim(c(1,35))+geom_text(data=r1,aes(x=q95,y=seq(0.1,0.9,length=5),label = paste(formatC(q95)),col=CV))+geom_vline(data=r1,aes(xintercept = q95,col=CV),lty=2)

ggplot(res1, aes(MDR,col=CV)) + geom_density()+xlim(c(0,35))+scale_x_log10()


res2 <- simCVn(cvs=cvs,mean_sim=1,p=c(0.5,0.5),Nsim=Nsim)

c2 <-res2%>%group_by(CV)%>% nest() %>% mutate(confusion=purrr::map(data,function(df) sapply(mdrs,function(x) sum(df>x)/Nsim)))%>% mutate(Mixture=2) %>% dplyr::select(-data) %>% unnest(cols=c(confusion)) %>% ungroup %>% mutate(cutoff=rep(mdrs,length(cvs))) %>% pivot_wider(names_from = cutoff,names_prefix="MDR>",values_from=confusion)

r2 <- res2%>%group_by(CV)%>% summarise(mean=mean(MDR),q95=quantile(MDR,0.95))%>% mutate(Mixture=2)
# ggplot(res2, aes(MDR,col=CV)) + stat_ecdf(geom = "point")+xlim(c(1,7))+ylab("")
res2$CV <- plyr::mapvalues(res2$CV,from=c(0.3,0.6,1,1.4,3),to=paste0(cvs*100,"%"))
r2$CV <- plyr::mapvalues(r2$CV,from=c(0.3,0.6,1,1.4,3),to=paste0(cvs*100,"%"))
res2$CV <- factor(res2$CV,levels=unique(res2$CV))
r2$CV <- factor(r2$CV,levels=unique(r2$CV))

p_sim_H0 <- ggplot(res2, aes(MDR,col=CV)) + stat_ecdf(geom = "step")+xlim(c(1,8))+ylab("")+geom_text(data=r2,aes(x=q95,y=seq(0.1,0.9,length=5),label = paste(formatC(q95)),col=CV))+geom_vline(data=r2,aes(xintercept = q95,col=CV),lty=2)
p_sim_H0


ggplot(res2, aes(MDR,col=CV)) + geom_density()+scale_x_log10()+geom_vline(data=r2,aes(xintercept = q95,col=CV),lty=2)+geom_text(data=r2,aes(x=q95,y=seq(0.5,2,length=5),label = paste(formatC(q95)),col=CV))
```


```{r}
res2 <- simCVn(cvs=cvs,mean_sim=1,p=c(0.5,0.5),Nsim=Nsim,reduction = 0.9)

c2 <-res2%>%group_by(CV)%>% nest() %>% mutate(confusion=purrr::map(data,function(df) sapply(mdrs,function(x) sum(df>x)/Nsim)))%>% mutate(Mixture=2) %>% dplyr::select(-data) %>% unnest(cols=c(confusion)) %>% ungroup %>% mutate(cutoff=rep(mdrs,length(cvs))) %>% pivot_wider(names_from = cutoff,names_prefix="MDR>",values_from=confusion)

r2 <- res2%>%group_by(CV)%>% summarise(mean=mean(MDR),q95=quantile(MDR,0.95))%>% mutate(Mixture=2)
# ggplot(res2, aes(MDR,col=CV)) + stat_ecdf(geom = "point")+xlim(c(1,7))+ylab("")
res2$CV <- plyr::mapvalues(res2$CV,from=c(0.3,0.6,1,1.4,3),to=paste0(cvs*100,"%"))
r2$CV <- plyr::mapvalues(r2$CV,from=c(0.3,0.6,1,1.4,3),to=paste0(cvs*100,"%"))
res2$CV <- factor(res2$CV,levels=unique(res2$CV))
r2$CV <- factor(r2$CV,levels=unique(r2$CV))
p_sim_HA <- ggplot(res2, aes(MDR,col=CV)) + stat_ecdf(geom = "step")+xlim(c(1,8))+ylab("")+geom_text(data=r2,aes(x=q95,y=seq(0.1,0.9,length=5),label = paste(formatC(q95)),col=CV))+geom_vline(data=r2,aes(xintercept = q95,col=CV),lty=2)
p_sim_HA
```

```{r}
gridExtra::grid.arrange(p_sim_H0,p_sim_HA)
```



```{r}
res3 <- simCVn(cvs=cvs,mean_sim=1,p=c(1/3,1/3,1/3),Nsim=Nsim)


c3 <-res3%>%group_by(CV)%>% nest() %>% mutate(confusion=purrr::map(data,function(df) sapply(mdrs,function(x) sum(df>x)/Nsim)))%>% mutate(Mixture=3) %>% dplyr::select(-data) %>% unnest(cols=c(confusion)) %>% ungroup %>% mutate(cutoff=rep(mdrs,length(cvs))) %>% pivot_wider(names_from = cutoff,names_prefix="MDR>",values_from=confusion)

r3 <- res3%>%group_by(CV)%>% summarise(mean=mean(MDR),q95=quantile(MDR,0.95))%>% mutate(Mixture=3)

## ggplot(res3, aes(MDR,col=CV)) + stat_ecdf(geom = "point")+xlim(c(1,7))+ylab("")
ggplot(res3, aes(MDR,col=CV)) + stat_ecdf(geom = "step")+xlim(c(1,15))+ylab("")+geom_text(data=r3,aes(x=q95,y=seq(0.1,0.9,length=5),label = paste(formatC(q95)),col=CV))+geom_vline(data=r3,aes(xintercept = q95,col=CV),lty=2)
ggplot(res3, aes(MDR,col=CV)) + geom_density()+scale_x_log10()+geom_text(data=r3,aes(x=q95,y=seq(0.1,2,length=5),label = paste(formatC(q95)),col=CV))+geom_vline(data=r3,aes(xintercept = q95,col=CV),lty=2)
```


```{r}
rbind(c1,c2,c3) %>% knitr::kable(.,digits=3, format = "pipe")
confusion <- rbind(c1,c2,c3) 
confusion <- confusion %>% pivot_longer(cols = starts_with("MDR"))
ggplot(confusion,aes(x=name,y=value,col=CV))+geom_point()+geom_line(aes(group=CV))+facet_grid(.~Mixture)+theme(axis.text.x = element_text(angle = 90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")
```



```{r}
res <- rbind(r1,r2,r3)
d95 <- res %>% select(-mean) %>% pivot_wider(names_from = CV,names_prefix="CV=",values_from=q95) ## %>% knitr::kable(.)
```



```{r}
dmean <- res%>% select(-q95) %>% pivot_wider(names_from = CV,names_prefix="CV=",values_from=mean) ## %>% knitr::kable(.)

```

```{r}
knitr::kable(
  list(dmean, d95),
  caption = 'Mean and 95th %-tile',
  booktabs = TRUE, valign = 't',format = "simple",digits = 3
)
```

## Confusion matrix and quantiles for MDR

```{r}
Res_Null <- simConfusion (cvs=c(0.3,0.6,1,1.4,3),
                         mdrs=c(1.25,2,3,5),
                         Nsim=12000,
                         synergy=F,reduction=0.5,q=0.95)
ggplot(Res_Null$confusion,aes(x=name,y=value,col=CV))+geom_point()+geom_line(aes(group=CV))+facet_grid(.~Mixture)+theme(axis.text.x = element_text(angle = 90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")

Res_Alt <- simConfusion (cvs=c(0.3,0.6,1,1.4,3),
                         mdrs=c(1.25,2,3,5),
                         Nsim=12000,
                         synergy=T,reduction=0.5,q=0.95)

ggplot(Res_Alt$confusion,aes(x=name,y=value,col=CV))+geom_point()+geom_line(aes(group=CV))+facet_grid(.~Mixture)+theme(axis.text.x = element_text(angle = 90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")
Res_Alt$confusion$Truth <- "synergy"
Res_Null$confusion$Truth <- "No synergy"
tmp <- rbind(Res_Null$confusion,Res_Alt$confusion)%>%pivot_wider(names_from = Truth,values_from=value)%>%mutate(Decision="Reject H0")

confusion <- rbind(tmp,tmp %>% mutate(`No synergy`=1-`No synergy`,synergy=1-synergy,Decision="Do not reject H0"))

conf <- confusion %>% group_by(Mixture,CV,name) %>% nest() %>% mutate(data=purrr::map(data,function(df){
  df[2:1,c(3,1,2)]%>%mutate(`No synergy`=paste0(c("True Negative=","False Positive="),formatC(`No synergy`*100,digits = 1,format="f"),"%"),synergy=paste0(c("False Negative=","True Positive="),formatC(synergy*100,digits = 1,format="f"),"%"))
}))%>%unnest(cols = c(data))
library(kableExtra)

conf %>% knitr::kable(., "markdown")
```


```{r}
conf %>% knitr::kable(., "html") %>% collapse_rows
```



```{r}
Res_Alt <- simConfusion (cvs=c(0.3,0.6,1,1.4,3),
                         mdrs=c(1.25,2,3,5),
                         Nsim=12000,
                         synergy=T,reduction=0.8,q=0.95)

ggplot(Res_Alt$confusion,aes(x=name,y=value,col=CV))+geom_point()+geom_line(aes(group=CV))+facet_grid(.~Mixture)+theme(axis.text.x = element_text(angle = 90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")
Res_Alt$confusion$Truth <- "synergy"

Res_Null$confusion$Truth <- "No synergy"
tmp <- rbind(Res_Null$confusion,Res_Alt$confusion)%>%pivot_wider(names_from = Truth,values_from=value)%>%mutate(Decision="Reject H0")

confusion <- rbind(tmp,tmp %>% mutate(`No synergy`=1-`No synergy`,synergy=1-synergy,Decision="Do not reject H0"))

conf <- confusion %>% group_by(Mixture,CV,name) %>% nest() %>% mutate(data=purrr::map(data,function(df){
  df[2:1,c(3,1,2)]%>%mutate(`No synergy`=paste0(c("True Negative=","False Positive="),formatC(`No synergy`*100,digits = 1,format="f"),"%"),synergy=paste0(c("False Negative=","True Positive="),formatC(synergy*100,digits = 1,format="f"),"%"))
}))%>%unnest(cols = c(data))
library(kableExtra)

conf %>% knitr::kable(., "markdown")
```

```{r}
conf %>% knitr::kable(., "html") %>% kable_paper(full_width = F) %>% collapse_rows
```




```{r}
Res_Alt <- simConfusion (cvs=c(0.3,0.6,1,1.4,3),
                         mdrs=c(1.25,2,3,5),
                         Nsim=12000,
                         synergy=T,reduction=0.9,q=0.95)

ggplot(Res_Alt$confusion,aes(x=name,y=value,col=CV))+geom_point()+geom_line(aes(group=CV))+facet_grid(.~Mixture)+theme(axis.text.x = element_text(angle = 90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")
Res_Alt$confusion$Truth <- "synergy"
Res_Null$confusion$Truth <- "No synergy"
tmp <- rbind(Res_Null$confusion,Res_Alt$confusion)%>%pivot_wider(names_from = Truth,values_from=value)%>%mutate(Decision="Reject H0")

confusion <- rbind(tmp,tmp %>% mutate(`No synergy`=1-`No synergy`,synergy=1-synergy,Decision="Do not reject H0"))

conf <- confusion %>% group_by(Mixture,CV,name) %>% nest() %>% mutate(data=purrr::map(data,function(df){
  df[2:1,c(3,1,2)]%>%mutate(`No synergy`=paste0(c("True Negative=","False Positive="),formatC(`No synergy`*100,digits = 1,format="f"),"%"),synergy=paste0(c("False Negative=","True Positive="),formatC(synergy*100,digits = 1,format="f"),"%"))
}))%>%unnest(cols = c(data))
library(kableExtra)

conf %>% knitr::kable(., "markdown")
```

```{r}
conf %>% knitr::kable(., "html") %>% kable_paper(full_width = F) %>% collapse_rows
```

## Cutoff for MDRs

- What should be the cutoff value for mis-classification? 5%? 
- Note that the variance of MDR based on simulation should be also considered. 

```{r}
df <- simCVmat(cvs=c(0.6,1,1.4),mean_sim=1,p=c(0.5,0.5),Nsim=5000,M=100)
df1 <- df%>% pivot_longer(cols = starts_with("Rep"),names_to="Replicate",values_to="MDR")
ggplot(df1,aes(x=MDR,col=CV)) + geom_density(aes(group=Replicate))+scale_x_log10()+facet_grid(.~CV)

MDRdist <- df1 %>% group_by(CV,Replicate) %>% summarise(mean=mean(MDR),q95=quantile(MDR,0.95))

skimr::skim(MDRdist)
MDRdist1 <- MDRdist %>% pivot_longer(cols = c(mean,q95),names_to="stats",values_to="value")
ggplot(MDRdist1,aes(x=CV,y=value))+geom_boxplot()+geom_point()+facet_grid(.~stats)#+scale_y_log10()

```



```{r}

df <- simCVmat(cvs=c(0.6,1,1.4),mean_sim=1,p=c(0.5,0.5),Nsim=5000,M=100,synergy=T)
df1 <- df%>% pivot_longer(cols = starts_with("Rep"),names_to="Replicate",values_to="MDR")
ggplot(df1,aes(x=MDR,col=CV)) + geom_density(aes(group=Replicate))+scale_x_log10()+facet_grid(.~CV)

MDRdist <- df1 %>% group_by(CV,Replicate) %>% summarise(mean=mean(MDR),q95=quantile(MDR,0.95))

skimr::skim(MDRdist)
MDRdist1 <- MDRdist %>% pivot_longer(cols = c(mean,q95),names_to="stats",values_to="value")
ggplot(MDRdist1,aes(x=CV,y=value))+geom_boxplot()+geom_point()+facet_grid(.~stats)#+scale_y_log10()


```


```{r}
tmp <- simConfusion_2mixture(mdrs = seq(1,10,by=0.02))
ggplot(tmp$confusion,aes(x=MDR,y=value,col=CV))+geom_point()+geom_line(aes(group=CV))+facet_grid(.~Mixture)+theme(axis.text.x = element_text(angle = 90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")

factorV <- c(2,3,5,10)
reductionV <- 1-1/factorV
mdrs <- seq(1,10,by=0.02)


resAll <- lapply(1:length(reductionV),function(i){
  tmp1 <- simConfusion_2mixture(mdrs = mdrs,synergy = T,reduction = reductionV[i])
  return(tmp1)
})
resAll0 <- list()
resAll0[[1]] <- tmp
resAll <- c(resAll0,resAll)
names(resAll) <- c(1,factorV)  #paste0("reduction=",c(0,reductionV))
resDF <- plyr::ldply(lapply(resAll,function(x)x$confusion))
resDF$.id <- factor(resDF$.id,levels=c(1,2,3,5,10))
resDF$reduction.factor <- resDF$.id
resDF$CV <-  plyr::mapvalues(resDF$CV,from=c(0.3,0.6,1,1.4,3),to=paste0(cvs*100,"%"))
resDF$CV <- factor(resDF$CV,levels=unique(resDF$CV))
p1 <- ggplot(resDF,aes(x=MDR,y=value,col=reduction.factor))+geom_point(pch=".")+geom_line(aes(group=.id))+facet_grid(.~CV)+theme(axis.text.x = element_text(angle = -90))+ scale_y_continuous(labels = scales::percent_format(accuracy = 1))+geom_hline(yintercept = 0.05,lty=2)+xlab("MDR Cutoff")+geom_hline(yintercept = 0.80,lty=2,col="darkblue")+theme(legend.position = "top")+guides(title="Reduction Factor")+geom_vline(xintercept = c(1.25,2,3,5),lty=3,col="darkblue")+scale_x_continuous(breaks = c(2,3,5,7.5,10))+ylab("")+see::scale_color_metro_d()
if(interactive())ggsave("inst/manuscript/MDRcut-off.png")

```


```{r}
library(patchwork)
theme_set(theme_bw())
(p_sim_H0+theme(legend.position = "top"))/p1+ plot_annotation(tag_levels = 'A')
if(interactive())ggsave("inst/manuscript_beemixtox/figure4.png",width=7,height=7,dpi=300)
```


Another idea could be finding the intersection of 0.05 of the ctable. 

https://stackoverflow.com/questions/52655729/get-x-value-given-y-value-general-root-finding-for-linear-non-linear-interpol


```{r}
## given (x, y) data, find x where the linear interpolation crosses y = y0
## the default value y0 = 0 implies root finding
## since linear interpolation is just a linear spline interpolation
## the function is named RootSpline1
RootSpline1 <- function (x, y, y0 = 0, verbose = TRUE) {
  if (is.unsorted(x)) {
     ind <- order(x)
     x <- x[ind]; y <- y[ind]
     }
  z <- y - y0
  ## which piecewise linear segment crosses zero?
  k <- which(z[-1] * z[-length(z)] <= 0)
  ## analytical root finding
  xr <- x[k] - z[k] * (x[k + 1] - x[k]) / (z[k + 1] - z[k])
  ## make a plot?
  if (verbose) {
    plot(x, y, "l"); abline(h = y0, lty = 2)
    points(xr, rep.int(y0, length(xr)))
    }
  ## return roots
  xr
  }
```

For cubic interpolation splines returned by stats::splinefun with methods "fmm", "natrual", "periodic" and "hyman", the following function provides a stable numerical solution.

```{r}
RootSpline3 <- function (f, y0 = 0, verbose = TRUE) {
  ## extract piecewise construction info
  info <- environment(f)$z
  n_pieces <- info$n - 1L
  x <- info$x; y <- info$y
  b <- info$b; c <- info$c; d <- info$d
  ## list of roots on each piece
  xr <- vector("list", n_pieces)
  ## loop through pieces
  i <- 1L
  while (i <= n_pieces) {
    ## complex roots
    croots <- polyroot(c(y[i] - y0, b[i], c[i], d[i]))
    ## real roots (be careful when testing 0 for floating point numbers)
    rroots <- Re(croots)[round(Im(croots), 10) == 0]
    ## the parametrization is for (x - x[i]), so need to shift the roots
    rroots <- rroots + x[i]
    ## real roots in (x[i], x[i + 1])
    xr[[i]] <- rroots[(rroots >= x[i]) & (rroots <= x[i + 1])]
    ## next piece
    i <- i + 1L
    }
  ## collapse list to atomic vector
  xr <- unlist(xr)
  ## make a plot?
  if (verbose) {
    curve(f, from = x[1], to = x[n_pieces + 1], xlab = "x", ylab = "f(x)")
    abline(h = y0, lty = 2)
    points(xr, rep.int(y0, length(xr)))
    }
  ## return roots
  xr
  }
```



# Methods

## CI for CV

### Naive method

$$\mbox{lcl} =
           \frac{s}{\bar{x}} \sqrt{\frac{n-1}{\chi^{2}_{(1-\alpha/2;n-1)}}}$$
$$\mbox{ucl} =
           \frac{s}{\bar{x}} \sqrt{\frac{n-1}{\chi^{2}_{(\alpha/2;n-1)}}}$$
           
### McKay confidence limit

$$
\mbox{lcl} = \frac{K}
              {\sqrt{\left( \frac{u_1}{n} - 1 \right) K^2 + \frac{u_1}{n-1}}}
$$

$$
\mbox{ucl} = \frac{K}
              {\sqrt{\left( \frac{u_2}{n} - 1 \right) K^2 + \frac{u_2}{n-1}}}
$$
where, $u_1 = \chi^2_{1 - \alpha/2,n-1}, u_2 = \chi^2_{\alpha/2,n-1}$

Note that if the coefficient of variation is greater than 0.33, either the normality of the data is suspect or the probability of negative values in the data is non-neglible. In this case, McKay's approximation may not be valid. Also, it is generally recommended that the sample size should be at least 10 before using McKay's approximation.

### Vangel proposed the following modification to McKay's method.

$$
\mbox{lcl} = \frac{K}
              {\sqrt{\left( \frac{u_1 + 2}{n} - 1 \right)
               K^2 + \frac{u_1}{n-1}}}
$$
$$
\mbox{ucl} = \frac{K}
              {\sqrt{\left( \frac{u_2 + 2}{n} - 1 \right)
              K^2 + \frac{u_2}{n-1}}}
$$
In general, the Vangel's modified McKay method is recommended over the McKay method. It generally provides good approximations as long as the data is approximately normal and the coefficient of variation is less than 0.33.


### Lognormal distribution

$$
\mbox{lcl} = \sqrt{\exp(a_L) - 1}
$$


$$
\mbox{lcl} = \sqrt{\exp(a_U) - 1}
$$

where, $a_L=\frac{(n-1)S_{n}^{2}} {\chi_{n-1,1-\alpha/2}}, a_U=\frac{(n-1)S_{n}^{2}} {\chi_{n-1,1-\alpha/2}}$, $S_{n}^2$ = the variance of the log of the data. 


