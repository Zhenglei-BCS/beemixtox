---
title: "msPAF"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{msPAF}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval=T,
  fig.width = 12,
  fig.height = 10
)
```



## Background

PAF(potentially affected fraction) definition: inverse way of $HC_p$. Given a concentration, find out $p$. In general, PAF can be considered an indicator for "toxic pressure" on the ecosystems.

The quantile (or PAF) for each species was calculated approximately by $\frac{i-0.5}{n}$, where $i$ is the rank of the sensitivity, $i=1$, the most sensitive and $n$ the least sensitive species.

$\alpha$ intercept, $\beta$ slope.


if assuming log-logistic distribution.

$$PAF(x)=1/(1+\exp((\alpha-x)/\beta))$$
$\alpha$ is the mean value of the log-logistic distribution, $\beta$ is the scale parameter. $x$ is log of the concentration. $\beta=\sigma\times\sqrt{3}/\pi$

Rewriting above into $x=\alpha-\beta\log(\frac{1-PAF}{PAF})$, when $x=HC_5$ then $PAF=0.05$


The current method to calculate the $HC_5$ and $HC_{50}$ makes use of the log-normal
species sensitivity distribution (Aldenberg and Jaworska, 2000) instead of a log-logistic
distribution. The differences between these two distributions are small. *The advantage of analytic tractability of the logistic distribution is outweighed by the
statistical advantage of using the log-normal distribution. (Personally I don't understand this sentence)*

Assuming log-normal

$$\log(HC_p)=x-k\cdot s$$

where $x$ is the mean of the log-transformed data, $k$ is the extrapolation constant depending on protection level and sample size (Aldenberg and Jaworska, 2000), $s$ is the standard deviation of log-transformed data. 

### msPAF (multisubstance PAF)

An aggregation protocol has been designed to aggregate single-substance PAF
values to a single overall term of ecological risk of a mixture. The protocol is based
on application of two toxicological concepts: concentration addition (termed Simple
Similar Action by Plackett and Hewlett, 1952) and response addition (termed Independent
Joint Action by Plackett and Hewlett, 1952). The concepts are summarized
and the associated rules of calculus are introduced below.


### Concentration Addition 

The toxicological concept of concentration addition has been defined for compounds
that have the same toxic mode of action and that show no toxicological interactions
(Plackett and Hewlett, 1952). This concept is usually applied to characterize the
response observed in single-species toxicity tests in which the response to several
compounds with the same mode of action is considered. By definition, the joint
effect of such compound mixtures can be calculated using (relative) concentration
addition rules of calculus (Plackett and Hewlett, 1952; Deneer et al., 1988a; Altenburger
et al., 2000; Chapter 15).

Neutral hydrophobic organic compounds are believed to act by a similar, nonspecific
toxic mode of action called *narcosis*, which every hydrophobic compound
can exert, but which can be masked by another, more specific toxic effect (Könemann,
1981). In other words, almost every hydrophobic chemical exerts at least a
narcotic, nonspecific toxicity that contributes to the joint toxicity of mixtures, and
this is often referred to as baseline toxicity (Verhaar et al., 1992; 1995). At the species level, toxicity equivalence factors (TEFs) have been used to express the toxicity of one compound as a fraction of another compound with the same toxic mode of
action. Transfer of the TEF principle to SSDs by scaling compounds in a similar
way results in so-called hazard units (Box 16.1). For compounds with the same toxic
mode of action, a single SSD can be derived using (relative) concentration addition
quantified by hazard units, representing the separate compounds and any mixture of
these compounds. It is assumed that the toxic mode of action in this case applies to
the SSD comprising all relevant species.

For compounds with a non-narcotic toxic mode of action, the situation is more
complicated. Some species experience the same type of effect due to specific interactions at targets sites that only occur in a fraction of the species, and other species only experience narcosis, due to lack of specific target sites for the compound. This holds, for example, for organophosphate (OP) biocides. Each OP biocide acts by
acetylcholinesterase inhibition, and thus all compounds in this group can be seen as
fractions of each other provided that the exposed organisms have a receptor for this
compound. For those organisms, concentration addition rules of calculus should be
applied for the specifically working OP biocides. However, species that lack the
target receptor are not sensitive for OP exposure, and will only experience narcotic
baseline toxicity. This means that concentration addition for specific toxic modes of
action is only appropriate within a single SSD if the SSD consists of species having
the specific receptor. For the organisms without the receptor, it may be more useful
to consider narcotic concentration addition. Moreover, compounds can be more toxic
than expected from baseline toxicity, although the exposed species do not have the
receptor for a compound. These complications are further elaborated in Chapter 22.

As a method to aggregate several PAF values from SSDs for compounds with
a similar toxic mode of action to a single msPAF term, it is proposed to apply
concentration addition rules. The cumulative density function (CDF) of (log) toxicity
data is regarded as similar to the log-dose–response curve for single species. This
is illustrated in Box 16.1 for NOECs, but the principle can be applied to other test
endpoints as well.

**Toxicity Data**

1. Obtain NOECs for each compound.
2. mg/L for aquatic organisms, mg/L soil pore water for soil organisms.
3. scale into dimensionless HU (hazard units), with 1 HU defined as the concentration where the NOEC is exceeded for 50% of all species tessted. 
 $$\bar NOEC_i^j=\frac{NOEC_i^j}{\bar x_i}$$
 where, $\bar NOEC_i^j$ is the scaled NOECs in HUs ($\mu$g/L/$\mu$g/L). $\bar x_i$ the median NOEC for substance $i$. $i=1, ..., n$, and species $j$ from $1,...,m$.

4. This is analogous to toxic unit (TU) scaling for a single species where 1 TU is the concentration causing 50% effect. 
5. Fitting SSD with either log-logistic model or log-normal in HUs for each compound. Alternatively, it can be found by shifting the CDF of log NOECs by subtracting log($\bar x_i$). 
6. Concentration addition in the context of SSDs implies that **SSDs have
similar variance** for compounds with the same toxic mode of action. The hypothesis is $H_0: \sigma_A=\sigma_B$. If they differ significantly, no rules of calculus
have as yet been developed to take divergence in variance (equivalent to
slope divergence in the classical concentration addition literature; see
Bliss, 1939; Plackett and Hewlett, 1952; De March, 1987) into account.
If they do not differ, this does not prove concentration additivity; it can
also be a result of small sample sizes.

**Exposure Data**

7. For each compound the exposure conentration is recalculated to HU.

**PAF Calculations for Concentration Addition (CA)**
8. The $msPAF_{CA}$ is read from the CDF by HU addition (non-log-transformed)
for a single toxic mode of action (TMoA):

$$HU_{TMoA}=\sum_i HU$$
9. If the log toxicity data are fitted to a logistic distribution, the PAF for each compound is given by 

$$PAF_i=\frac{1}{1+e^{-(x-\alpha)/\beta}}$$

with $\alpha$ mean of the log toxicity data and $\beta=\sigma\cdot\sqrt 3/\pi$, $\sigma$ the sd and $x$ the log of the exposure concentration.

10. After recalculation of all compound concentrations to HUs, $\alpha=0$ by definition and $x$ is given by the sum of $HU_{TMoA}$ of all compounds with the same toxic mode of action. The $\beta$ is averaged over the compounds or, alternatively, can be derived from a database analysis for the toxic mode of action.

11. the $msPAF_{CA}$ for a group of substances
with the same toxic mode of action is calculated as

$$PAF_{TMoA}=\frac{1}{1+e^{-\log(\sum HU_{TMoA}))/\beta_{TMoA}}}$$

### Response Addition (Not used in the calculation)

For compounds that have different modes of action, and that do not interact at the
target site of toxic action, mixture toxicity can be described by the term
response addition (RA), initially referred to as Independent (Joint) Action (Bliss, 1939; Plackett and Hewlett, 1952). This concept is usually applied to characterize the response observed in single-species toxicity tests for chemicals with dissimilar modes of action. A correlation of sensitivities to the different compounds is absent.

**PAF Calculations for Response Addition**

- The combination effect for compounds with different modes of action is
calculated analogous to the probability of two nonexcluding processes
(Hewlett and Plackett, 1979), where a species is affected by compound A
and/or B:

$$P(A \cup B) = P(A)+P(B)-P(A\cap B)$$
- For $P(A\cap B)$ These are related to the covariation of sensitivities, denoted by
r.

- When $r=0$, then $P(A\cap B)=P(A)\cdot P(B)$, which leads to 

$$PAF_{RA}=P(A)+P(B)-P(A)\cdot P(B)
$$
- To simplify for  multiple compound mixtures. The nonaffected fraction, is
introduced. If $Q_A=1-PAF_A$, and $Q_B=1-PAF_B$, the nonaffected fraction
of the mixture of A and B is a part of $Q_A$ of $Q_B$. 

$$Q_{AB}=Q_A\cdot Q_B
$$

- Then msPAF is given by:
$$
PAF_{RA}=1-Q_{AB}=1-(1-PAF_A)(1-PAF_B)
$$
- For more compounds

$$
PAF_{RA}=1-\Pi_i(1-PAF_i)
$$
### Aggregation to msPAF

The substances comprising the mixture are grouped in three different
types: (a) one or more groups of compounds in which there is within-group concentration
addition, (b) groups of compounds distinguished by between-group
response addition, and (c) remaining compounds with a toxic mode of action that
is unique for the given mixture. Calculation of overall msPAF of all the compounds
from the three different types a to c then proceeds in the following order:


1. SSDs are calculated for each compound, yielding as many (single-substance)
SSDs as there are compounds on the list.
2. Concentration addition (according to Box 16.1) is applied to each group
of compounds with the same toxic mode of action, to aggregate single
PAFs for compounds of type a, showing within-group concentration addition.
This yields msPAF values based on concentration addition ($msPAF_{CA}$)
for as many modes of action as are distinguished in the mixture.
3. Response addition according to Box 16.2 is applied to the $msPAF_{CA}$ values
obtained in step 2 with the remaining single PAF values (aggregating over
types a to c).


## Toxic Pressure Modelling

$$\mbox{Toxic Pressure} = \Pr(C_w>EC_{50}) = \int_{-\infty}^{\infty}pdf(zC_w)\times CDF(zEC_{50})dz$$
with $z(C_w)=\frac{\log C_w-av(\log EC_{50})}{sd(\log EC_{50})}$ and 

$z(EC_{50})=\frac{\log EC_{50}-av(\log EC_{50})}{sd(\log EC_{50})}$, where $pdf(zC_w)$ and $CDF(zEC_{50})$ represent the pdf of toxicologically standardized exposure concentrations (in water) and the cdf of toxicologically standardized acute $EC_{50}$ values. 

Values of $sd(\log EC_{50}$ are often not
known with great precision for many chemicals, because of
insufficient numbers of experimental toxicity data (Posthuma
et al. 2019b). Following the practice of Posthuma et al. (2019b),
we assigned the value 0.7 to across‐species standard
deviations of all individual chemicals, for the purpose of toxic
pressure calculation.

## Question from Ismael

b.	I calculated the msPAF according to the equation, and checked if the sum of the individual PAF values (calculated for each chemical individually) is equal to the calculated msPAF for the mixture. In fact the sum(PAF) underestimate the msPAF and is dependent on n components. I think it is logical because the equation involves operations in log scales and CDFs, so that msPAF(1+2) is not equal to PAF1 + PAF2. 
c.	The question is: How can I calculate the fractional contribution of each chemical X to the msPAF? Do you think it is possible? Or may there be a magical mathematical approximation to solve this problem?

**My thoughts**

similar to the frequent practice of water quality assessment for mixtures by *linearly summed risk quotients over compounds*, whereby numerical values estimated
with the simplified and the original (more complex) model are similar.


Relative rankings of the contributions of chemicals to those impacts on a regional scale
were derived in 2 steps. In the first step, *a relative toxicity score for each compound* in a subset of samples (Europe or a specific example river basin) was determined as the product of the *mean compound toxic pressure* in the set and *the ratio between the non-zero values for that compound and the non-zero values for the mixtures*. Those represent the magnitude and the relative contribution and frequency of increased compound pressures to total mixture pressures. All scores were then relatively ranked using the lowest-ranking compound as the baseline (defined as “1”).

A PAF value relates to the concept of protective
benchmarks, if those are derived from an SSD of chronic
NOECs, such as the hazardous concentration for 5% of the
species (HC5). Higher toxic pressures imply higher fractions of
species affected for the studied acute or chronic endpoint.

sufficient protection relates to PAF-NOECmax=
msPAF-NOECmax=0.05, which is in regulatory terms considered
to protect 95% of the species against adverse effects.


The observed Pareto-type
(skewed) distributions imply that relatively few sites are
characterized by relatively high chronic toxic pressures.

toxic pressure data aggregated over an area and over time.


## Reproducible R Script and Output


### Data Preprocessing 

- Load in the data needed. 

```{r include=F}

## Note for this block it is not necessary to include in the output file since the preprocessing of data will also be included. 
library(beemixtox)
data("Data.AggBySiteID.2")
data("Data.SSD")
data("chemclass")
# Dir_Data = "~/s3raw/WB-Zhenglei/"
# CAS.Metals = read.csv(paste0(Dir_Data,"CAS.Metals.csv"))
# CAS.Metals$CAS = as.character(CAS.Metals$CAS)
# CAS.Priority = read.csv(paste0(Dir_Data,"EU_WFD_45PriorityPollutants.csv"))
# CAS.Priority$CAS = gsub("CAS_|-|_", "", CAS.Priority$CAS)
# CAS.Priority$CAS = as.character(CAS.Priority$CAS)



# How many entries from the existing data we will be able to use?
CAS.SSD = unique(Data.AggBySiteID.2$CAS)[unique(Data.AggBySiteID.2$CAS)%in%as.character(unique(Data.SSD$CAS.))]

Data.SSD.1 = Data.SSD[Data.SSD$CAS.%in%CAS.SSD,]; Data.SSD.1 = droplevels(Data.SSD.1)


#LOQ.type = c("LOQ.T0", "LOQ.T1")
#Exclusion.CAS = c("Excluded_None", "Excluded_Metals", "Current_Use", "Metals", "Banned_Listed")




```


- take only Lake water body and River water body data.
- SSD data from L.Posthuma SSDs
- Keep only "total water" to avoid replications
- remove rows with Missing observed value ("O" category) and unreliable meta info ("U" category).
- Remove 85 rows with measurement has been confirmed by country to be taken from a highly polluted area 



- The values that are below LOQ are set to 0 $\mu$g/L.
- Exclude priority polluants entries and metals.
- Note here we can make it a parameterized report or using functions. 

```{r}
library(tidyverse)
LOQ.type <- "LOQ.T0"  # The values that are below LOQ are set to 0 ug/L
dataset.name <- LOQ.type
Exclusion.CAS <- "Current_Use"
Exclusion.CAS <- "Excluded_Metals"


Data.AggBySiteID.3 <-  selectdata(Data.AggBySiteID.2,chemclass,LOQ.type = LOQ.type,
                       Exclusion.CAS = Exclusion.CAS)
# New name to be able to restart for running other options ? legacy part from Ismael
Data.AggBySiteID.3a <-  Data.AggBySiteID.3

```







```{r}
#########################################################################################
# Recalculate Measured and detected chemicals based on the exclussions

# How many determinads measured per station and Year?
#variable to keep
var = c("monitoringSiteIdentifier","CAS","phenomenonTimeReferenceYear")
DataForNChem.1 = Data.AggBySiteID.3a[,names(Data.AggBySiteID.3a)%in%c(var)]
## Note by ZG: Could directly take the column names 
## DataForNChem.2 <- Data.AggBySiteID.3a[,c("monitoringSiteIdentifier","CAS","phenomenonTimeReferenceYear")]
#measured chemicals

```


```{r ismael-aggregate,eval=F,include=F}
measured.chem.1 = aggregate(CAS ~ monitoringSiteIdentifier + phenomenonTimeReferenceYear, data =DataForNChem.1, length) ## Get number of CAS measured for each site and year. 
#detected chem
detected.chem.1 = aggregate(CAS ~ monitoringSiteIdentifier + phenomenonTimeReferenceYear, data =DataForNChem.1[Data.AggBySiteID.3a$resultMaximumValue!=0,], length)

#Subset data for these selection of Station Year combinations
measured.chem.1$Site.Year = paste(measured.chem.1$monitoringSiteIdentifier,measured.chem.1$phenomenonTimeReferenceYear, sep=".")
detected.chem.1$Site.Year = paste(detected.chem.1$monitoringSiteIdentifier,detected.chem.1$phenomenonTimeReferenceYear, sep=".")

# Merge measured & detected
measured.chem.1 = merge(measured.chem.1, detected.chem.1, by=c("monitoringSiteIdentifier","phenomenonTimeReferenceYear","Site.Year"), all.x = T)
names(measured.chem.1)[c(4,5)] = c("N.Measured.1", "N.Detected.1")
measured.chem.1$N.Detected.1[is.na(measured.chem.1$N.Detected.1)] = 0 #There are some NAs that needs attention, may be are samples where all is non-detected?
measured.chem.1$Ratio.MD.1 = measured.chem.1$N.Detected.1/measured.chem.1$N.Measured.1
summary(measured.chem.1)

Data.AggBySiteID.3a = merge(Data.AggBySiteID.3a,measured.chem.1, by=c("monitoringSiteIdentifier","phenomenonTimeReferenceYear","Site.Year"), all.x = T)
measured.chem.All = merge(measured.chem, measured.chem.1, by=c("monitoringSiteIdentifier","phenomenonTimeReferenceYear","Site.Year"), all.x = T)
# No measure, no detection to 0
measured.chem.All$N.Measured.1[is.na(measured.chem.All$N.Measured.1)] = 0
measured.chem.All$N.Detected.1[is.na(measured.chem.All$N.Detected.1)] = 0
summary(measured.chem.All)
hist(measured.chem.All$N.Measured.1)
hist(measured.chem.All$N.Detected.1)
table(measured.chem.All$N.Measured.1>10) # 9407 combinations measuring 10 or more ==> 9602
table(measured.chem.All$N.Detected.1>10) # 2135 combinations detecting 10 or more ==> 2289
# Merge with SSD info (mu.sig) by CAS numner, note each CAS has one SSD info!
Data.AggBySiteID.3a = merge(Data.AggBySiteID.3a, Data.SSD.1[,names(Data.SSD.1)%in%c("CAS.","X10LogSSDMedianConcentration.ug.L..MuAcute.EC50","X10LogSSDMedianConcentration.ug.L..MuChronic.NOEC", "X10LogSSDSlope.ug.L..SigmaAcute.EC50","X10LogSSDSlope.ug.L..SigmaChronic.NOEC")], by.x = "CAS", by.y="CAS.")
names(Data.AggBySiteID.3a)[(16:19)] = c("SSDLOG10.Mu.Acute.EC50", "SSDLOG10.Sigma.Acute.EC50", "SSDLOG10.Mu.chronic.NOEC","SSDLOG10.Sigma.chronic.NOEC")
```

- use monitoring site, Year as grouping variables to cacluated the number of measured, the number of detected, and the ratio as detection rate. 
- left join the aggregated dataset with SSD information with CAS being the identifier.

```{r}

tmp <- Data.AggBySiteID.3a %>% group_by(monitoringSiteIdentifier,phenomenonTimeReferenceYear) %>% summarise(Nmeasured=length(CAS),Ndetected=sum(resultMaximumValue>0),Ratio.MD=Ndetected/Nmeasured) ## Ratio is detection rate from all measured. 
# Merge
Data.AggBySiteID.3a <- left_join(Data.AggBySiteID.3a,tmp)


### check there is no NAs
summary(Data.AggBySiteID.3a$resultMaximumValue)
summary(Data.AggBySiteID.3a$resultMeanValue)
## Read in ssd information
ssdInfo <- Data.SSD.1[,c("CAS.","X10LogSSDMedianConcentration.ug.L..MuAcute.EC50","X10LogSSDMedianConcentration.ug.L..MuChronic.NOEC", "X10LogSSDSlope.ug.L..SigmaAcute.EC50","X10LogSSDSlope.ug.L..SigmaChronic.NOEC")] %>% mutate(CAS.=as.character(CAS.))
Data.AggBySiteID.3a <- left_join(Data.AggBySiteID.3a,ssdInfo,by=c("CAS"="CAS."))
dim(Data.AggBySiteID.3a) ## [1] 348635     19

names(Data.AggBySiteID.3a)[(16:19)] = c("SSDLOG10.Mu.Acute.EC50", "SSDLOG10.Mu.chronic.NOEC", "SSDLOG10.Sigma.Acute.EC50","SSDLOG10.Sigma.chronic.NOEC")
```


### PAF Calculation 

- For each row, calculated acute $HC_{50}$ and Chronic $HC_{5}$, the hazard quotient is then calculated as the maximum concentration divided by acute $HC_{50}$ and the mean concentration divided by Chronic $HC_{5}$. 

```{r}
# Calculate PAF by row.
# Equivalent to excel NORMDIST(log10[sumHU],0,0.7,1) is pnorm(log10(Sum(HU),0,07,log=F)
# Carefull because Mu and sigma are given in log10 monitoring data is given in linear scale
# It is necessary to backtransform Mu values to linear scale!?

#Hazard.Index to the midpoint SSD by chem (reference to the HC05)
#Hazard.Index to the HC05 SSD by chem
# Taken from: https://edild.github.io/ssd/
# Will use compoud-specific slopes. Assuming 0.7 SSD slope is quite worst case (as how the data comes out)

#Compute HC05 by chemical ############################################################################
#The mu and sig parameters are given in log10 units!
Data.AggBySiteID.3a$HC50.Acute = 10^qnorm(0.5,Data.AggBySiteID.3a$SSDLOG10.Mu.Acute.EC50, Data.AggBySiteID.3a$SSDLOG10.Sigma.Acute.EC50)
Data.AggBySiteID.3a$HC05.Chronic = 10^qnorm(0.05,Data.AggBySiteID.3a$SSDLOG10.Mu.Acute.EC50, Data.AggBySiteID.3a$SSDLOG10.Sigma.chronic.NOEC)

Data.AggBySiteID.3a$HQ.Acute = Data.AggBySiteID.3a$resultMaximumValue/Data.AggBySiteID.3a$HC50.Acute
Data.AggBySiteID.3a$HQ.Chronic = Data.AggBySiteID.3a$resultMeanValue/Data.AggBySiteID.3a$HC05.Chronic

```


- Take a look at the distribution of the SSD slopes. 

```{r}
summary(Data.SSD.1$X10LogSSDSlope.ug.L..SigmaAcute.EC50)
plot(hist(Data.SSD.1$X10LogSSDSlope.ug.L..SigmaAcute.EC50))
a <- (hist(Data.SSD.1$X10LogSSDSlope.ug.L..SigmaAcute.EC50,breaks = 100))
a$breaks[which(a$density==max(a$density))] ## 0.68
a$mids[which(a$density==max(a$density))] ## 0.69
```



```{r}
which(is.na(Data.AggBySiteID.3a$HC50.Acute))
## note if not removing CAS with missing SSD information, then it could be cases with  X10LogSSDMedianConcentration.ug.L..MuAcute.EC50 = 0.93, but X10LogSSDSlope.ug.L..SigmaAcute.EC50 =NA
```

- Given the assumption is that all chemicals have equal slope SSD (which is not a valid assumption in real world), the acute and chronic mixture PAF can be calculated as "msPAF.Chronic.NOEC" and "msPAF.Acute.EC50". 

- Note that HQ.Acute is equal to HU.Acute.EC50. However, HQ.Chronic is not equal to HU.Chronic.NOEC, since Chronic HQ is calculated with HC5(0.05) whereas acute HQ is calculated with HC50. 

- Check the differences
```{r}
nrow(Data.AggBySiteID.3a)-sum(Data.AggBySiteID.3a$HQ.Chronic==Data.AggBySiteID.3a$HU.Chronic.NOEC)
Data.AggBySiteID.3a$HQ.Chronic[31]
#[1] 1.072166e-05
Data.AggBySiteID.3a$HU.Chronic.NOEC[31]
#[1] 1.994019e-06


nrow(Data.AggBySiteID.3a)-sum(Data.AggBySiteID.3a$HQ.Acute==Data.AggBySiteID.3a$HU.Acute.EC50)

```

=======




```{r}
# Summaries
summary(Data.AggBySiteID.3a$HQ.Acute)
summary(Data.AggBySiteID.3a$HQ.Chronic)

hist(log10(Data.AggBySiteID.3a$HQ.Acute))
hist(log10(Data.AggBySiteID.3a$HQ.Chronic))

#How many samples greater than HC05?
summary(Data.AggBySiteID.3a$HQ.Acute>1)
summary(Data.AggBySiteID.3a$HQ.Chronic>1)


# PAF by chemical ##################################################################################
# Calculate HU (Hazard Unit) Anti-log10 are taken on Mu because HUs needs to be calculated in log-non-tramsformed data.
Data.AggBySiteID.3a$HU.Acute.EC50 = Data.AggBySiteID.3a$resultMaximumValue/(10^Data.AggBySiteID.3a$SSDLOG10.Mu.Acute.EC50)
Data.AggBySiteID.3a$HU.Chronic.NOEC = Data.AggBySiteID.3a$resultMeanValue/(10^Data.AggBySiteID.3a$SSDLOG10.Mu.chronic.NOEC)
# Calculate PAF
Data.AggBySiteID.3a$PAF.Acute.EC50 = pnorm(log10(Data.AggBySiteID.3a$HU.Acute.EC50),0,Data.AggBySiteID.3a$SSDLOG10.Sigma.Acute.EC50,log=F) ## Note this is equal to pnorm(log10(Data.AggBySiteID.3a$resultMaximumValue),Data.AggBySiteID.3a$SSDLOG10.Mu.Acute.EC50 SSDLOG10.Sigma.Acute.EC50,)
# del <- Data.AggBySiteID.3a %>% mutate(PAF.Acute= pnorm(log10(resultMaximumValue),SSDLOG10.Mu.Acute.EC50,SSDLOG10.Sigma.Acute.EC50))
Data.AggBySiteID.3a$PAF.Chronic.NOEC = pnorm(log10(Data.AggBySiteID.3a$HU.Chronic.NOEC),0,Data.AggBySiteID.3a$SSDLOG10.Sigma.chronic.NOEC,log=F)

# Summaries
summary(Data.AggBySiteID.3a$PAF.Acute.EC50)
summary(Data.AggBySiteID.3a$PAF.Chronic.NOEC)

summary(Data.AggBySiteID.3a$PAF.Acute.EC50>0.05)
# Mode   FALSE    TRUE
# logical  243451    6495
sum(Data.AggBySiteID.3a$PAF.Acute.EC50>0.05)/nrow(Data.AggBySiteID.3a) #0.25% of samples ==>

summary(Data.AggBySiteID.3a$PAF.Chronic.NOEC>0.05)
sum(Data.AggBySiteID.3a$PAF.Chronic.NOEC>0.05)/nrow(Data.AggBySiteID.3a)

hist(log10(Data.AggBySiteID.3a$PAF.Acute.EC50))
hist(log10(Data.AggBySiteID.3a$PAF.Chronic.NOEC))
```


- Ismaels way of joining the the data. 

```{r}
HU.sum = aggregate(. ~ monitoringSiteIdentifier + Site.Year, data=Data.AggBySiteID.3a[,names(Data.AggBySiteID.3a)%in%c("monitoringSiteIdentifier","Site.Year","HU.Acute.EC50","HU.Chronic.NOEC", "HQ.Acute", "HQ.Chronic")], sum, na.rm=TRUE)
HQ.Max = aggregate(. ~ monitoringSiteIdentifier + Site.Year, data=Data.AggBySiteID.3a[,names(Data.AggBySiteID.3a)%in%c("monitoringSiteIdentifier","Site.Year","HU.Acute.EC50","HU.Chronic.NOEC","HQ.Acute", "HQ.Chronic", "PAF.Acute.EC50", "PAF.Chronic.NOEC")], max, na.rm=TRUE)


# Compute msPAF (multi-substance predicted affected fractions) Ismael's method
HU.sum$msPAF.Acute.EC50 = pnorm(log10(HU.sum$HU.Acute.EC50),0,mean(Data.AggBySiteID.3a$SSDLOG10.Sigma.Acute.EC50),log=F)
HU.sum$msPAF.Chronic.NOEC = pnorm(log10(HU.sum$HU.Chronic.NOEC),0,mean(Data.AggBySiteID.3a$SSDLOG10.Sigma.chronic.NOEC),log=F)

# Compute maxPAF (maximum PAF per site)
HQ.Max$maxPAF.Acute.EC50 = pnorm(log10(HQ.Max$HU.Acute.EC50),0,mean(Data.AggBySiteID.3a$SSDLOG10.Sigma.Acute.EC50),log=F)
HQ.Max$maxPAF.Chronic.NOEC = pnorm(log10(HQ.Max$HU.Chronic.NOEC),0,mean(Data.AggBySiteID.3a$SSDLOG10.Sigma.chronic.NOEC),log=F)

Data.msPAF = merge(HU.sum, HQ.Max, by = c("monitoringSiteIdentifier","Site.Year"))
names(Data.msPAF)
#  [1] "monitoringSiteIdentifier" "Site.Year"                "HQ.Acute.x"              
#  [4] "HQ.Chronic.x"             "HU.Acute.EC50.x"          "HU.Chronic.NOEC.x"       
#  [7] "msPAF.Acute.EC50"         "msPAF.Chronic.NOEC"       "HQ.Acute.y"              
# [10] "HQ.Chronic.y"             "HU.Acute.EC50.y"          "HU.Chronic.NOEC.y"       
# [13] "PAF.Acute.EC50"           "PAF.Chronic.NOEC"         "maxPAF.Acute.EC50"       
# [16] "maxPAF.Chronic.NOEC"   
names(Data.msPAF)[c(3,4,9,10)] = c("HI.HC50.Acute.EC50","HI.HC05.Chronic.NOEC", "MaxHQ.HC50.Acute.EC50","MaxHQ.HC05.Chronic.NOEC")

```


```{r}

median(Data.AggBySiteID.3a$SSDLOG10.Sigma.chronic.NOEC)
median(Data.AggBySiteID.3a$SSDLOG10.Sigma.Acute.EC50)

```

- calculate MCR = HQ_Sum/HQ_Max (check if using HQ or HU!)


- group by site and year, calculating sum of HU, sum of HQ, max of HU, max of HQ. 


```{r}
Res <- Data.AggBySiteID.3a %>%  group_by(monitoringSiteIdentifier,phenomenonTimeReferenceYear,Site.Year) %>% summarise(HU_Sum_Acute=sum(HU.Acute.EC50,na.rm=T),HU_Sum_Chronic=sum(HU.Chronic.NOEC,na.rm=T),HQ_Sum_Acute=sum(HQ.Acute,na.rm=T),HQ_Sum_Chronic=sum(HQ.Chronic,na.rm=T),HU_Max_Acute=max(HU.Acute.EC50,na.rm=T),HU_Max_Chronic=max(HU.Chronic.NOEC,na.rm=T),HQ_Max_Acute=max(HQ.Acute,na.rm=T),HQ_Max_Chronic=max(HQ.Chronic,na.rm=T),PAF_Max_Acute=max(PAF.Acute.EC50,na.rm=T),PAF_Max_Chronic=max(PAF.Chronic.NOEC,na.rm=T),Nmeasured=length(CAS),Ndetected=sum(resultMaximumValue>0),Ratio.MD=Ndetected/Nmeasured) %>% mutate(HI_Acute=HQ_Sum_Acute,HI_Chronic=HQ_Sum_Chronic,MaxHQ.Acute=HQ_Max_Acute,MaxHQ.Chronic=HQ_Max_Chronic) ## Ratio

nrow(Res)- sum(Res$HU_Sum_Acute==Res$HQ_Sum_Acute)
nrow(Res)- sum(Res$HQ_Sum_Acute==Res$HI_Acute)
nrow(Res)- sum(Res$HQ_Sum_Chronic==Res$HI_Chronic)

nrow(Res)- sum(Res$HQ_Max_Acute==Res$MaxHQ.Acute)
nrow(Res)- sum(Res$HQ_Max_Chronic==Res$MaxHQ.Chronic)

```


- calculating msPAF with sum or max of **HU**. 

```{r}
Res <- Res %>% mutate(msPAF_Sum_Acute=pnorm(log10(HU_Sum_Acute),0,0.7),msPAF_Max_Acute=pnorm(log10(HU_Max_Acute),0,0.7),msPAF_Sum_Chronic=pnorm(log10(HU_Sum_Chronic),0,0.7),msPAF_Max_Chronic=pnorm(log10(HU_Max_Chronic),0,0.7)) 
```

- `MCR = HQ_sum/HQ_max`
- `HI = HQ_sum ; maxHQ = HQ_max`

```{r}
Res <- Res %>% mutate(MCR.HC05.Chronic.NOEC = HQ_Sum_Chronic/HQ_Max_Chronic,MCR.HC50.Acute.EC50=HQ_Sum_Acute/HQ_Max_Acute) 
```

- Calculations by site. 

```{r}
sum(Res$HU_Max_Acute>1,na.rm = T)/length(Res$HQ_Max_Acute)
sum(Res$HU_Max_Acute>0.1,na.rm = T)/length(Res$HQ_Max_Acute)


sum(Res$msPAF_Sum_Acute>0.05,na.rm = T)/length(Res$HQ_Max_Acute)
sum(Res$msPAF_Sum_Acute>0.1,na.rm = T)/length(Res$HQ_Max_Acute)




Res <- Res %>% ungroup %>% group_by(monitoringSiteIdentifier) %>% mutate(maxHI_site=max(HQ_Sum_Chronic,na.rm=T))  ## Aggregate over "Year" 


# > quantile(Data.msPAF$HI.HC05.Chronic.NOEC, c(0.5, 0.9, 0.95), na.rm = T)
#         50%         90%         95% 
# 0.003408162 0.255350147 0.379421665 
quantile(Res$HQ_Sum_Chronic, c(0.5, 0.9, 0.95), na.rm = T)
quantile(Res$HU_Sum_Chronic, c(0.5, 0.9, 0.95), na.rm = T) ## Note that HU Chronic does not equal to HQ Chronic as HQ use HC5 whereas HU use mean (HC50)!!
quantile(Res$msPAF_Sum_Chronic, c(0.5, 0.9, 0.95), na.rm = T)
quantile(Res$MCR.HC05.Chronic.NOEC, c(0.5, 0.9, 0.95), na.rm = T)

quantile(Res$HQ_Sum_Acute, c(0.5, 0.9, 0.95), na.rm = T)
quantile(Res$HU_Sum_Acute, c(0.5, 0.9, 0.95), na.rm = T)
quantile(Res$msPAF_Sum_Acute, c(0.5, 0.9, 0.95), na.rm = T)
quantile(Res$MCR.HC50.Acute.EC50, c(0.5, 0.9, 0.95), na.rm = T)       
## Note that in Ismael's original script, he also keeps another dataset where only the maximum HQ_Sum substance for each site is kept.     
```


Use CEFIC-MIAT decision tree table to group data. 

## CEFIC-MIAT decision tree table (Valloton &Proce 2016) 

- Group I: Risk driven by single chemicals HI >1 & MaxHQ >1
- Group II:No concern HI < 1, MaxHQ < 1
- Group IIIA: Mixtures concern driven by 1 or 2 chemicals. HI>1 & Max HI <1 & MCR < 2
- Group IIIB: Mixture risk. HI > 1 & MaxHQ < 1 & MCR >2


```{r}
HI_cut <- c(1,0.33,0.1)
HQ_max_cut <- c(1,0.33,0.1)
MCR_cut <- 2

get_group_1 <- function(HI,HQ_max,MCR,AF=c(1,3,10)){
  cutoff <- 1/AF
  
  if(HI>cutoff & HQ_max > cutoff ) {
    gr <- "I"
  }else{
    if(HI<=cutoff & HQ_max <= cutoff) gr <-"II" else{
      if(HI > cutoff & HQ_max <= cutoff){
        if(MCR <= 2) gr <- "IIIA" else gr <- "IIIB"
      }else{
        gr <- NA
      }
    } 
  }
  return(gr)
}

get_group <- function(HIv,HQ_maxv,MCRv,AF){
  sapply(1:length(HIv), function(x)get_group_1(HIv[x],HQ_maxv[x],MCRv[x],AF))
}

Res <- Res %>% ungroup %>% mutate(Group_AF1_Chronic=get_group(HI=HQ_Sum_Chronic,HQ=HQ_Max_Chronic,MCR=MCR.HC05.Chronic.NOEC,AF=1),Group_AF3_Chronic=get_group(HI=HQ_Sum_Chronic,HQ=HQ_Max_Chronic,MCR=MCR.HC05.Chronic.NOEC,AF=3),Group_AF10_Chronic=get_group(HI=HQ_Sum_Chronic,HQ=HQ_Max_Chronic,MCR=MCR.HC05.Chronic.NOEC,AF=10),Group_AF1_Acute=get_group(HI=HQ_Sum_Acute,HQ=HQ_Max_Acute,MCR=MCR.HC50.Acute.EC50,AF=1),Group_AF3_Acute=get_group(HI=HQ_Sum_Acute,HQ=HQ_Max_Acute,MCR=MCR.HC50.Acute.EC50,AF=3),Group_AF10_Acute=get_group(HI=HQ_Sum_Acute,HQ=HQ_Max_Acute,MCR=MCR.HC50.Acute.EC50,AF=10))


table(Res$Group_AF1_Chronic)
table(Res$Group_AF1_Acute)
```

```{r eval=F}
# save.image("~/Projects/beemixtox/msPAF.RData")
if(interative()){
  
  Dir_Results = "raw-data/Waterbase_Analysis/LOQ.T0/"
  dataset.name = basename(Dir_Results)
  setwd(Dir_Results)
}
```

## Table for the Manuscript

Table 1. CEFIC-MIAT Table 

- Include only AF 1 (Do we want to include other Afs?) 
- Chronic, same for Acute (for SM Materials) 
- LOQ.type = c("LOQ.T0", "LOQ.T1") 
- Exclusion.CAS = c("Excluded_None", "Excluded_Metals", "Current_Use") 
- Generate identical Table for Acute for SM. 



## Figures for the Manuscript


```{r}
library(beemixtox)
data("Data.AggBySiteID.2")
data("Data.SSD")
data("chemclass")
CAS.SSD = unique(Data.AggBySiteID.2$CAS)[unique(Data.AggBySiteID.2$CAS)%in%as.character(unique(Data.SSD$CAS.))]
Data.SSD.1 = Data.SSD[Data.SSD$CAS.%in%CAS.SSD,]; Data.SSD.1 = droplevels(Data.SSD.1)

```

### Figure 1

Figure 1. log10(HI)-MCR Plots. Faceted plot (2x3). 

Chronic, same for Acute (for SM Materials) 

LOQ.type = c("LOQ.T0", "LOQ.T1") 

Exclusion.CAS = c("Excluded_None", "Excluded_Metals", "Current_Use") 

To do: 

- Check the function defining the regions (Group II, Group I, Group IIIa, Group IIIb) 

- Add annotation with the name of the group and % of samples 

- Add annotation with the N.Site_Year, N.CAS 

- Add colored sharing to the no concern group 


**Note that 1808 site.year has HI of 0, around 15.5 %**

```{r}
recting <- rbind(data.frame(xmin=-Inf,xmax=1,ymin=-Inf,ymax=Inf,col="green"),
                 data.frame(xmin=1,xmax=Inf,ymin=0,ymax=2,col="red"),
                  data.frame(xmin=1,xmax=Inf,ymin=2,ymax=Inf,col="yellow"))
Res <- getResData(Data.AggBySiteID.2,chemclass,LOQ.type = c("LOQ.T0","LOQ.T1"),
            Exclusion.CAS = c("Excluded_None","Excluded_Metals","Current_Use"),Data.SSD.1)

sum(Res$HI_Acute==0 & Res$Exclusion.CAS=="Excluded_None" & Res$LOQ.type=="LOQ.T0")
sum(Res$HI_Acute==0 & Res$Exclusion.CAS=="Excluded_None" & Res$LOQ.type=="LOQ.T1")

df <- data.frame(x=1e-6,y=15)

N_CAS_Site <- Res %>% group_by(LOQ.type,Exclusion.CAS) %>% summarise(N.Site=length(unique(monitoringSiteIdentifier)),N.Sample=length(monitoringSiteIdentifier))
N.CAS <- sapply(c("Excluded_None","Excluded_Metals","Current_Use"),function(x)getNcas(x,chemclass))
N_CAS_Site$N.CAS <- rep(N.CAS[unique(N_CAS_Site$Exclusion.CAS)],2)
N_CAS_Site <- data.frame(df,N_CAS_Site)

Nclass <- Res %>% group_by(LOQ.type,Exclusion.CAS,Group_AF1_Acute) %>% summarise(N=length(monitoringSiteIdentifier))%>% left_join(N_CAS_Site[,c("LOQ.type","Exclusion.CAS","N.Sample")])%>% mutate(Perc=N/N.Sample*100)
Nclass1 <- Nclass %>% filter(Group_AF1_Acute=="IIIA" | Group_AF1_Acute =="IIIB")%>% group_by(LOQ.type,Exclusion.CAS) %>% summarise(Perc=sum(Perc)) %>% mutate(Group="III")

Nclass <- rbind(Nclass1,Nclass[,c("LOQ.type","Exclusion.CAS","Perc","Group_AF1_Acute")]%>%rename(Group=Group_AF1_Acute)%>%filter(Group=="I"|Group=="II"))

df1 <- data.frame(x=1e-6,y=10,Label="No concern",Group="II")
df1 <- rbind(df1,data.frame(x=20,y=5,Label="Single Substance Risk",Group="I"))
df1 <- rbind(df1,data.frame(x=5,y=20,Label="Multiple Chemicals Risk",Group="III"))
Nclass <- Nclass %>% left_join(df1)
#ggplot(Res,aes(x=log10(HI_Acute),y=MCR.HC50.Acute.EC50))+geom_point()+geom_text(data=df,aes(x=x,y=y),label=paste0("N.Site = ",10,"\nN.CAS = ",5))+facet_grid(LOQ.type~Exclusion.CAS)
ggplot(Res,aes(x=HI_Acute,y=MCR.HC50.Acute.EC50))+geom_point(aes(col=Group_AF1_Acute))+geom_text(data=N_CAS_Site,aes(x=x,y=y,label=paste0("N.Site = ",N.Site,"\nN.CAS = ",N.CAS)))+geom_text(data=Nclass,aes(x=x,y=y,label=paste0(Group,"\n",formatC(Perc,digits = 2,format="f"),"%")),vjust="inward") +scale_x_log10()+geom_vline(xintercept = 1,col="blue")+geom_hline(yintercept = 2,col="blue")+facet_grid(LOQ.type~Exclusion.CAS,scale="free")+ annotate("rect", xmin = 1e-10, xmax = 1, ymin = recting[1,3], ymax = recting[1,4],fill="yellow",alpha=0.2)+geom_line(data=data.frame(x=seq(1,20,length=100),y=seq(1,20,length=100)),aes(x=x,y=y),col="blue")
# geom_rect(data=recting,aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax,fill=col))
# ggplot(data.frame(x=1:10,y=1:10),aes(x=x,y=y))+geom_point()+scale_x_log10()+ geom_line() 
if(interactive()){
  ggsave("inst/manuscript/Figure1-Acute.png",width = 15,height=8,dpi=300)
}

N_CAS_Site$y <- 12


Nclass <- Res %>% group_by(LOQ.type,Exclusion.CAS,Group_AF1_Chronic) %>% summarise(N=length(monitoringSiteIdentifier))%>% left_join(N_CAS_Site[,c("LOQ.type","Exclusion.CAS","N.Sample")])%>% mutate(Perc=N/N.Sample*100)
Nclass1 <- Nclass %>% filter(Group_AF1_Chronic=="IIIA" | Group_AF1_Chronic =="IIIB")%>% group_by(LOQ.type,Exclusion.CAS) %>% summarise(Perc=sum(Perc)) %>% mutate(Group="III")

Nclass <- rbind(Nclass1,Nclass[,c("LOQ.type","Exclusion.CAS","Perc","Group_AF1_Chronic")]%>%rename(Group=Group_AF1_Chronic)%>%filter(Group=="I"|Group=="II"))

df1 <- data.frame(x=1e-6,y=10,Label="No concern",Group="II")
df1 <- rbind(df1,data.frame(x=50,y=5,Label="Single Substance Risk",Group="I"))
df1 <- rbind(df1,data.frame(x=5,y=16,Label="Multiple Chemicals Risk",Group="III"))
Nclass <- Nclass %>% left_join(df1)


ggplot(Res,aes(x=HI_Chronic,y=MCR.HC05.Chronic.NOEC))+geom_point(aes(col=Group_AF1_Chronic))+geom_text(data=N_CAS_Site,aes(x=x,y=y,label=paste0("N.Site = ",N.Site,"\nN.CAS = ",N.CAS)))+geom_text(data=Nclass,aes(x=x,y=y,label=paste0(Group,"\n",formatC(Perc,digits = 2,format="f"),"%")),vjust="inward")  +scale_x_log10()+geom_vline(xintercept = 1)+facet_grid(LOQ.type~Exclusion.CAS,scale="free")+ annotate("rect", xmin = 1e-10, xmax = 1, ymin = recting[1,3], ymax = recting[1,4],fill="yellow",alpha=0.2)+geom_line(data=data.frame(x=seq(1,16,length=100),y=seq(1,16.5,length=100)),aes(x=x,y=y))+scale_y_continuous(limits=(c(1,16)),expand = c(0,0))
if(interactive()){
  ggsave("inst/manuscript/Figure1-Chronic.png",width = 12,height=8,dpi=300)
}
```

### Figure 2

Figure 2.  log10(HI)-N.Chemicals Measured & Detected. Faceted plot (2x4?). 

Chronic, same for Acute (for SM Materials) 

LOQ.type = c("LOQ.T0", "LOQ.T1") 

Exclusion.CAS = c("Excluded_None" (for N.Measured), "Excluded_None", "Excluded_Metals", "Current_Use") 

To do: 

-Add annotation with the name of the group and % of samples (only Group II, no concern) 
-Add annotation with the N.Site_Year, N.CAS 
-Add colored sharing to the no concern group 


```{r}
df <- data.frame(x=1e-5,y=75)

tmp <- Res[,c("HI_Acute","Ndetected","Nmeasured","LOQ.type","Exclusion.CAS")]

ggplot(Res,aes(x=HI_Acute,y=Ndetected))+geom_point()+scale_x_log10()+geom_vline(xintercept = 1)+facet_grid(LOQ.type~Exclusion.CAS,scale="free") +geom_text(data=df,aes(x=x,y=y),label=paste0("N.Site = ",10,"\nN.CAS = ",5))

ggplot(Res,aes(x=HI_Acute,y=Nmeasured))+geom_point()+scale_x_log10()+geom_vline(xintercept = 1)+facet_grid(LOQ.type~Exclusion.CAS,scale="free") +geom_text(data=df,aes(x=x,y=y),label=paste0("N.Site = ",10,"\nN.CAS = ",5))


```


### Figure 3

Figure 3.  
- HI vs Station (Stacked bar plot) HI by 1th, 2th, 3th, and all Other components (for Station_Year with HI > 0.1) 
- Boxplot (inside the Stacked bar plot) with the % Risk by 1th, 2th, 3th, all other component. 
- Faceted plot (2x2) 
- Chronic, same for Acute (for SM Materials) 

- LOQ.type = c("LOQ.T0", "LOQ.T1") 
- Exclusion.CAS = c("Excluded_Metals", "Current_Use") 


*check the main drivers! breakdown the HI into driver 1, 2, 3 and other.*

```{r}
driverdata <- getDriverData(Data.AggBySiteID.2,chemclass,LOQ.type = c("LOQ.T0", "LOQ.T1"),
                       Exclusion.CAS = c("Excluded_Metals", "Current_Use"), Data.SSD.1)

ggplot(driverdata,aes(x=Site.Year,y=HQ))+geom_bar(aes(fill=Component,col=Component),position="stack",stat = "identity")+facet_grid(LOQ.type~Exclusion.CAS+test)
sumdriver <- driverdata %>% group_by(LOQ.type,Exclusion.CAS,test,Component) %>% summarise(HQ=sum(HQ)) %>% ungroup %>% group_by(LOQ.type,Exclusion.CAS,test) %>% mutate(sumHQ=sum(HQ),Perc=HQ/sumHQ) %>% mutate(Component=factor(Component,levels=c("Driver 1","Driver 2","Driver 3","All Other" )))+theme()

ggplot(sumdriver,aes(x=Component,y=Perc))+geom_bar(aes(fill=Component),stat="identity")+facet_grid(LOQ.type~Exclusion.CAS+test)+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


driverdata1 <- driverdata %>% group_by(LOQ.type,Exclusion.CAS,test) %>% nest()%>% mutate(data=map(data,function(df){
  df1<- df%>%group_by(Site.Year,monitoringSiteIdentifier,phenomenonTimeReferenceYear,HI) %>% nest()
  
  df <- df1[order(df1$HI,decreasing = T),]%>% add_column(x=1:nrow(df1))%>%unnest(cols = c(data))%>% ungroup
  return(df)}))%>%unnest(cols = c(data))%>%mutate(Component=factor(Component,levels=c("Driver 1","Driver 2","Driver 3","All Other" )))

ggplot(driverdata1,aes(x=x,y=HQ))+geom_bar(aes(fill=Component,col=Component),position=position_stack(reverse = T),stat = "identity")+facet_wrap(LOQ.type~Exclusion.CAS+test,scale="free")+ theme(axis.text.x = element_blank(), axis.ticks = element_blank())


 # annotation_custom(grob = ggplotGrob(an_sub), 
 #                          xmin = 5400000, xmax = 6600000, 
 #                          ymin = 2950000, ymax = 4150000)
# - https://ikashnitsky.github.io/2017/subplots-in-maps/
if(interactive()){
  ggsave("inst/manuscript/Figure3.png",width = 12,height=8,dpi=300)
}

```


### Figure 4 (barplot)

Figure 4. Main risk drivers 

There are two potential options for this plot 

- Option 1 (geom_bar):  

- Building on the same approach used in Figure 3. We investigate only the chemicals that appear more frequently among the first, second or third risk drivers for the Station_Year observations with HI > 0.1. 

- Pros: We build on previous analysis, and focus only in the chemicals that are most relevant for stations with the larger His. 
- Cons: Does not provide a more general assessment across all chemicals of those that could be of potential relevance (independently on whether they appear or not very frequently in the most risky mixtures). The frequency of appearance depends on (1) concentration (2) frequency of monitoring. This is uncaptured by this analysis. 

Plot: 

- Chronic, identical plot for acute for SM. 
- Frequency (%) as first, second or third component ~ CAS 
- Faceting: (1 x 2) 
- LOQ.type = c("LOQ.T0", "LOQ.T1") 
- Exclusion.CAS = c("Excluded_Metals") 
- Color by Percentile (as is now) 

```{r}
sumCAS <- driverdata %>% group_by(LOQ.type,Exclusion.CAS,test)%>%mutate(nSample=length(unique(Site.Year))) %>% group_by(LOQ.type,Exclusion.CAS,test,CAS,nSample) %>%nest() %>% mutate(nappear=map(data,nrow))%>% unnest(cols=c(nappear))%>% mutate(freq=nappear/nSample ) %>% select(-data) %>% group_by(LOQ.type,Exclusion.CAS,test)%>% nest() %>% mutate(data=map(data,function(df){
  df <- df[order(df$freq,decreasing = T),]
  df <- df%>%add_column(x=1:nrow(df))
  return(df)
}))%>%unnest(cols = c(data))%>%ungroup #%>% mutate(x=paste0(x,"-",CAS))

sumCAS <- sumCAS %>% left_join(chemclass[,c("CAS","Substance","Chem.Group")]%>%mutate(CAS=as.character(CAS)))%>%filter(CAS!="")%>% filter(x<=31) %>% mutate(Sub1=paste0(x,freq,"*",Substance)) %>% mutate(Sub1=factor(Sub1,levels=rev(unique(Sub1))))

ggplot(sumCAS%>%filter(Exclusion.CAS=="Excluded_Metals" ),aes(x=Sub1,y=freq,fill=Chem.Group))+geom_bar(stat="identity")+ coord_flip()+facet_wrap(LOQ.type~test,scale="free",drop=TRUE)+scale_x_discrete(breaks=levels(sumCAS$Sub1),label=str_split(levels(sumCAS$Sub1),"\\*",simplify=T)[,2])+scale_y_continuous(labels = scales::percent_format(accuracy = 1))

#ggplot(sumCAS%>%filter(Exclusion.CAS=="Excluded_Metals" & test=="Acute"&LOQ.type=="LOQ.T0"),aes(x=reorder(Substance,freq),y=freq,fill=Chem.Group))+geom_bar(stat="identity")+ coord_flip()+facet_wrap(LOQ.type~test,scale="free",drop=TRUE)
if(interactive()){
  ggsave("inst/manuscript/Figure4a.png",width = 12,height=8,dpi=300)
}
```


### Figure 4a (dotplot)

Option 2 (geom_point): 

We approach the relevance as HQ quantiles per CAS. Quantiles P95, P75, P50, P25, ordered by P95. We explore the frequency in which each CAS is found at RQ levels of reference (RQ = 0.01, 0.1, 1). Only the chemicals found beyond certain level, at certain frequency can have probability of being relevant overall for the mixtures. 

- Pros: Explore in a more detail way the potential contribution of all CAS to overall risk. Further, information on the frequency of monitoring and detection, and LOQs can be easily added to the plot for enhance transparency and interpretation. 
- Cons: Not directly linked to previous analysis (Figure 3). A little less direct to link single chemical relevance to "mixture effects". 

Plot: 

- Chronic, identical plot for acute for SM. 
- HQ ~ CAS (for Quantiles P95, P75, P50, P25, ordered by P95) 
- @Zhenglei. To add min(LOQ) per CAS to the plot with a different symbol 
- Faceting: (1 x 2) 
- LOQ.type = c("LOQ.T0", "LOQ.T1") 
- Exclusion.CAS = c("Excluded_Metals") 
- Color by "Chem.Group (Metal, Industrial, PAH, Pharma, Pesticide) 

```{r}
sumCAS <- driverdata %>% group_by(LOQ.type,Exclusion.CAS,test)%>%mutate(nSample=length(unique(Site.Year))) %>% group_by(LOQ.type,Exclusion.CAS,test,CAS,nSample) %>%nest() %>% mutate(nappear=map(data,nrow),QI=map(data,function(df) quantile(df$HQ,c(0.25,0.5,0.75,0.95))))%>% unnest(cols=c(nappear,QI))%>% mutate(freq=nappear/nSample ) %>% select(-data) %>% mutate(Quantile=c("25%","50%","75%","95%"))%>% group_by(LOQ.type,Exclusion.CAS,test,CAS)%>% mutate(q95=QI[Quantile=="95%"])%>% filter(q95>0.01)%>% group_by(LOQ.type,Exclusion.CAS,test)%>%nest()%>%mutate(data=map(data,function(df){
    df <- df %>% group_by(CAS,q95) %>% nest() 
    df <- df[order(df$q95),]
    df <- df %>% add_column(x=1:nrow(df))
    df <- df %>% unnest(cols=c(data))
    return(df)
}))%>%unnest(cols = c(data))%>%ungroup #%>% mutate(x=paste0(x,"-",CAS))

sumCAS <- sumCAS %>% left_join(chemclass[,c("CAS","Substance","Chem.Group")]%>%mutate(CAS=as.character(CAS)))%>%filter(CAS!="")#%>% filter(x<=31)
sumCAS<- sumCAS%>%mutate(Sub1=paste0(x,"*",Substance)) %>% mutate(Sub1=factor(Sub1,levels=unique(Sub1)))
## To add min(LOQ) per CAS to the plot with a different symbol 



ggplot(sumCAS%>%filter(Exclusion.CAS=="Excluded_Metals" ),aes(x=Sub1,y=QI,fill=Chem.Group))+geom_point(aes(col=Chem.Group))+ coord_flip()+facet_wrap(LOQ.type~test,scale="free",drop=TRUE)+scale_x_discrete(breaks=levels(sumCAS$Sub1),label=str_split(levels(sumCAS$Sub1),"\\*",simplify=T)[,2])
if(interactive()){
  ggsave("inst/manuscript/Figure4b.png",width = 12,height=15,dpi=300)
}


ggplot(sumCAS%>%filter(Exclusion.CAS=="Excluded_Metals" & test=="Acute" & LOQ.type=="LOQ.T0"),aes(x=reorder(Substance,q95),y=QI,col=Chem.Group))+geom_point(stat="identity")+ coord_flip()+facet_wrap(LOQ.type~test,scale="free",drop=TRUE)
```



## Make figures to illustrate the results

### ECDF plots with quantiles as vertical lines. 

- Note: HI (Sum of all RQ(HQ)s) 

```{r}
library(patchwork)
ecdf.Plots = list()
ecdf.Plot.1 = ggplot(data=Res, aes(HQ_Sum_Chronic)) +
  stat_ecdf(pad = F, geom ="point") +
  geom_vline(xintercept = c(quantile(Res$MCR.HC05.Chronic.NOEC, c(0.5, 0.9, 0.95), na.rm = T)), linetype = c(1,2,2), colour = c("black", "black","red")) +
  labs(x="HI.HC05.Chronic.NOEC", y="cumulative probability (p)", title= paste(paste0("N = ",nrow(Res)), dataset.name,Exclusion.CAS, sep = ", "))
# ecdf.Plot.1
ecdf.Plots[[1]] = ecdf.Plot.1


ecdf.Plot.2 = ggplot(data=Res, aes(HQ_Sum_Acute)) +
  stat_ecdf(pad = F, geom ="point") +
  geom_vline(xintercept = c(quantile(Res$MCR.HC50.Acute.EC50, c(0.5, 0.9, 0.95), na.rm = T)), linetype = c(1,2,2), colour = c("black", "black","red")) +
  labs(x="HI.HC05.Chronic.NOEC", y="cumulative probability (p)", title= paste(paste0("N = ",nrow(Res)), dataset.name,Exclusion.CAS, sep = ", "))
ecdf.Plots[[2]] = ecdf.Plot.2

ecdf.Plots[[1]] + ecdf.Plots[[2]] +plot_annotation(tag_levels = 'A')
if(interactive()) ggsave(paste("ecdf.HI.HC05NOEC_HC50EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=12, height = 5, units = "in")
```

- ECDF plots for MCR

```{r}
#ecdf plots for MCR
ecdf.Plot.3 = ggplot(data=Res, aes(MCR.HC05.Chronic.NOEC)) +
  stat_ecdf(pad = F, geom ="point") +
  geom_vline(xintercept = c(quantile(Res$MCR.HC05.Chronic.NOEC, c(0.5, 0.9, 0.95), na.rm = T)), linetype = c(1,2,2), colour = c("black", "black","red")) +
  labs(x="MCR.Chronic.NOEC", y="cumulative probability (p)", title= paste(paste0("N = ",nrow(Res)), dataset.name,Exclusion.CAS, sep = ", "))
# ecdf.Plot.3
ecdf.Plots[[3]] = ecdf.Plot.3
# ggsave(paste("comProb.MCR.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

ecdf.Plot.4 = ggplot(data=Res, aes(MCR.HC50.Acute.EC50)) +
  stat_ecdf(pad = F, geom ="point") +
  geom_vline(xintercept = c(quantile(Res$MCR.HC50.Acute.EC50, c(0.5, 0.9, 0.95), na.rm = T)), linetype = c(1,2,2), colour = c("black", "black","red")) +
  labs(x="MCR.Acute.EC50", y="cumulative probability (p)", title= paste(paste0("N = ",nrow(Res)), dataset.name,Exclusion.CAS, sep = ", "))
# ecdf.Plot.4
ecdf.Plots[[4]] = ecdf.Plot.4
# ggsave(paste("comProb.MCR.HC50.Acute.EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")


# Arrange the plots using patchworks syntax
ecdf.Plots[[3]] + ecdf.Plots[[4]] +
  # plot_layout(guides = 'collect') +
  plot_annotation(tag_levels = 'A')

if(interactive()) ggsave(paste("ecdf.MCR.HC05NOEC_HC50EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=12, height = 5, units = "in")

##########################
```

## Classical HI-MCR (CEFIC-MIAT DECISION TREE PLOTS)

- Note again HI chronic is 

```{r}
SumHI.MCR = list()
Plot.3 = ggplot(data=Res, aes(x = HQ_Sum_Chronic, y = MCR.HC05.Chronic.NOEC)) +
  geom_point() +
  labs(x="HI.HC05.Chronic.NOEC", y="MCR.Chronic.NOEC", title= paste("HI-MCR.Chronic.NOEC", dataset.name,Exclusion.CAS, sep =", "))
# Plot.3
SumHI.MCR[[1]] = Plot.3
# ggsave(paste("SumHI-MCR.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

#Acute
Plot.4 = ggplot(data=Res, aes(x = HQ_Sum_Acute, y =MCR.HC50.Acute.EC50)) +
  geom_point() +
  labs(x="SumHI.HC50.Acute.EC50", y="MCR.Acute.EC50", title= paste("HI-MCR.Acute.EC50", dataset.name,Exclusion.CAS, sep =", "))
# Plot.4
SumHI.MCR[[2]] = Plot.4
# ggsave(paste("SumHI-MCR.HC50.Acute.EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

#Combine
# Arrange the plots using patchworks syntax
SumHI.MCR[[1]] + SumHI.MCR[[2]] +
  # plot_layout(guides = 'collect') +
  plot_annotation(tag_levels = 'A')
if(interactive()) ggsave(paste("SumHI-MCR.HC05NOEC_HC05EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=12, height = 5, units = "in")

```

## PLOTS of MCR as a function of N.Measured and N.Detect

```{r}
# 
MCR.Meas.Det = list()
#CHR
#Measured
Plot.5 = ggplot(data=Res, aes(x = Nmeasured, y =MCR.HC05.Chronic.NOEC)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Measured", y="MCR.HC05.Chronic.NOEC", title= paste("N.Measured-MCR.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS, sep =", "))
# Plot.5
MCR.Meas.Det[[1]] = Plot.5
# ggsave(paste("N.Measured-MCR.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

#Detected
Plot.6 = ggplot(data=Res, aes(x = Ndetected, y =MCR.HC05.Chronic.NOEC)) +
  geom_point(alpha=0.3, stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), colour = "red") +
  # geom_quantile(quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Detected", y="MCR.HC05.Chronic.NOEC", title= paste("N.Detected-MCR.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS, sep =", "))
# Plot.6
MCR.Meas.Det[[2]] = Plot.6
# ggsave(paste("N.Detected-MCR.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

#Acute
Plot.7 = ggplot(data=Res, aes(x = Nmeasured, y =MCR.HC50.Acute.EC50)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Measured", y="MCR.HC50.Acute.EC50", title= paste("N.Measured-MCR.HC50.Acute.EC50", dataset.name,Exclusion.CAS, sep =", "))
# Plot.7
MCR.Meas.Det[[3]] = Plot.7
# ggsave(paste("N.Measured-MCR.HC50.Acute.EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")


#Detected
Plot.8 = ggplot(data=Res, aes(x = Ndetected, y =MCR.HC50.Acute.EC50)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Detected", y="MCR.HC50.Acute.EC50", title= paste("N.Detected-MCR.HC50.Acute.EC50", dataset.name,Exclusion.CAS, sep = ", "))
# Plot.8
MCR.Meas.Det[[4]] = Plot.8
# ggsave(paste("N.Detected-MCR.HC50.Acute.EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")


#Combine
# Arrange the plots using patchworks syntax
MCR.Meas.Det[[1]] + MCR.Meas.Det[[3]] + MCR.Meas.Det[[2]] + MCR.Meas.Det[[4]] +
  # plot_layout(guides = 'collect') +
  plot_layout(ncol = 2) +
  plot_annotation(tag_levels = 'A')
if(interactive()) ggsave(paste("N.Measured_N.Detected-MCR.HC05NOEC_HC05EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=12, height = 10, units = "in")

```

### Plots of HI (sumHQ) as a function of Nmeasured and Ndetected

```{r}
SumHI.Meas.Det = list()
Plot.9 = ggplot(data=Res, aes(x = Nmeasured, y =HQ_Sum_Chronic)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +
  scale_y_log10() +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Measured", y="log10(HI.HC05.Chronic.NOEC)", title= paste("N.Measured-log10(HI.HC05.Chronic.NOEC)", dataset.name,Exclusion.CAS, sep= ", "))
# Plot.9
SumHI.Meas.Det [[1]] = Plot.9

Plot.10 = ggplot(data=Res, aes(x = Ndetected, y =HQ_Sum_Chronic)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +  # stat_ecdf(pad = F, geom ="step") +
  scale_y_log10() +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Detected", y="log10(HI.HC05.Chronic.NOEC)", title= paste("N.Detected-log10(HI.HC05.Chronic.NOEC)", dataset.name,Exclusion.CAS, sep = ", "))
# Plot.10
SumHI.Meas.Det [[2]] = Plot.10

#Acute
Plot.11 = ggplot(data=Res, aes(x = Nmeasured, y =HQ_Sum_Acute)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +  # stat_ecdf(pad = F, geom ="step") +
  scale_y_log10() +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Measured", y="log10(HI.HC50.Acute.EC50)", title= paste("N.Measured-log10(HI.HC50.Acute.EC50)", dataset.name,Exclusion.CAS, sep=", "))
# Plot.11
SumHI.Meas.Det [[3]] = Plot.11


#Acute
Plot.12 = ggplot(data=Res, aes(x = Ndetected, y =HQ_Sum_Acute)) +
  geom_point(alpha=0.3,stat = "unique") +
  geom_hline(yintercept = 2, colour = "blue") +
  geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +  # stat_ecdf(pad = F, geom ="step") +
  scale_y_log10() +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="N.Detected", y="log10(HI.HC50.Acute.EC50)", title= paste("N.Detected-log10(HI.HC50.Acute.EC50)", dataset.name,Exclusion.CAS, sep = ", "))
# Plot.12
SumHI.Meas.Det [[4]] = Plot.12


#Combine
# Arrange the plots using patchworks syntax
SumHI.Meas.Det[[1]] + SumHI.Meas.Det[[3]] + SumHI.Meas.Det[[2]] + SumHI.Meas.Det[[4]] +
  # plot_layout(guides = 'collect') +
  plot_layout(ncol = 2) +
  plot_annotation(tag_levels = 'A')

if(interactive()) ggsave(paste("N.Measured_N.Detected-SumHI.HC05NOEC_HC05EC50", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=12, height = 10, units = "in")

```


### Plots of HI vs. MaxHQ

```{r}
Plot.13 = ggplot(data=Res, aes(x = HQ_Max_Chronic, y =HQ_Sum_Chronic)) +
  geom_point(alpha=0.3,stat = "unique") +
  # geom_hline(yintercept = 2, colour = "blue") +
  # geom_quantile(method = "rqss", quantiles = c(0.1, 0.5, 0.95), lambda = 1, colour = "red") +  # stat_ecdf(pad = F, geom ="step") +
  scale_y_log10() +
  scale_x_log10() +
  # facet_wrap(~Study.Type, scales = "fixed") +
  # coord_cartesian(xlim = c(-3, 3)) +
  labs(x="log10(MaxHQ.HC05.Chronic.NOEC)", y="log10(HI.HC05.Chronic.NOEC)", title= paste("Max.HI vs HI.HC05(Chronic.NOEC)", dataset.name,Exclusion.CAS, sep =", "))
# Plot.13
if(interactive())ggsave(paste("Max.HI vs HI.HC05.Chronic.NOEC", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")


```



## Summaries of Chemical Detection, HQ and Risk by CAS

```{r eval=F}
Data.AggBySiteID.4 <-  left_join(Data.AggBySiteID.3a[,c("CAS","monitoringSiteIdentifier", "AboveLOQ", "Site.Year", "HQ.Acute", "HQ.Chronic")],Res) %>% 
  mutate(Perc.Risk.Chr = 100*(HQ.Chronic/HQ_Sum_Chronic),
         Perc.Risk.Acute = 100*(HQ.Acute/HQ_Sum_Acute))                                

library(broom)
nested <- Data.AggBySiteID.4 %>% droplevels() %>% ungroup %>% group_by(CAS) %>% nest() 
SumRes  <- nested %>% mutate(N.Measured=map(data,~nrow(.x)),N.Detected=map(data,~sum(.x$HQ.Acute>0)),qt.HQ.Acute=map(data,~quantile(.x$HQ.Acute, probs = c(0.25,0.5,0.75,0.95))),qt.HQ.Chronic=map(data,~quantile(.x$HQ.Chronic, probs = c(0.25,0.5,0.75,0.95))),qt.Risk.Perc.Acute=map(data,~quantile(.x$Perc.Risk.Acute, probs = c(0.25,0.5,0.75,0.95),na.rm=T)), qt.Risk.Perc.Chronic=map(data,~quantile(.x$Perc.Risk.Chr, probs = c(0.25,0.5,0.75,0.95),na.rm=T))) 
SumRes$CAS.Type <- NA
# Nature of the chemical

SumRes$CAS.Type[SumRes$CAS%in%CAS.Metals$CAS] = "Metal"
SumRes$CAS.Type[SumRes$CAS%in%CAS.Priority$CAS] = "Priority.Non-Metal"
SumRes$CAS.Type[!SumRes$CAS%in%c(CAS.Priority$CAS,CAS.Metals$CAS)] = "Other"

# Add chemical name
Data.SSD.1$CAS. <- as.character(Data.SSD.1$CAS.)
SumRes = left_join(SumRes, Data.SSD.1[,c("CAS.","Substance","PrimaryMoA")], by=c("CAS"="CAS."))
k <- nrow(SumRes)

SumRes <- SumRes %>% unnest(c(N.Measured,N.Detected))%>% select(-data)
SumRes <- SumRes[order(SumRes$N.Detected),]
SumRes.plot <- SumRes %>% unnest(c(qt.HQ.Acute,qt.HQ.Chronic,qt.Risk.Perc.Acute,qt.Risk.Perc.Chronic), keep_empty = TRUE)  %>% ungroup()%>% mutate(qt=rep(c("25%","50%","75%","95%"),k)) %>% mutate(label.HQ.Acute=sapply(1:(k*4),function(x) ifelse(qt.HQ.Acute[x]>0.1,Substance[x],NA)),label.HQ.Chronic=ifelse(qt.HQ.Chronic>0.1,Substance,NA),label.Perc.Chronic=ifelse(qt.Risk.Perc.Chronic>10,Substance,NA),label.Perc.Acute=ifelse(qt.Risk.Perc.Chronic>10,Substance,NA))
```


```{r eval=F}
Scatter.Plots = list()
S.Plot.1<-ggplot(subset(SumRes.plot,qt=="95%"),aes(x=N.Measured,y=qt.HQ.Chronic, label = label.HQ.Chronic))+
  geom_point(aes(colour = CAS.Type))+
  geom_text(angle = 45, hjust = 0, nudge_x = 0.05, size = 3) +
  # geom_text() +
  # geom_hline(yintercept = c(0.1, 0.01), linetype = c(2,3)) +
  geom_hline(yintercept = c(1, 0.1, 0.001, 0.00001), linetype = c(1,2,2,2)) +
   scale_x_log10()+
  # scale_y_log10()+
  theme_classic()+
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"), plot.title = element_text(size = 10), strip.text.x =element_text(size = 14), rect = element_blank(), panel.border=element_rect(size=2,fill=NA, colour='black'),
        axis.text.y = element_text(size = 9), axis.text.x = element_text(size = 14), axis.title.x=element_text(size=12), axis.title.y=element_text(size=12), legend.text=element_text(size=14), legend.title=element_text(size=14),
        legend.key.size=unit(1, "cm"), legend.position="right", legend.box.background = element_rect(size=1,fill=NA))+
  labs(title= paste(paste0("HQ.Chronic.95P  ","N.CAS = ",nrow(SumRes)), dataset.name,Exclusion.CAS, sep = ", "))
Scatter.Plots[[1]] = S.Plot.1
# S.Plot.1
# ggsave(paste("Scatter.CAS.HQ.Chronic.95P-N.Measured", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

S.Plot.2<-ggplot(subset(SumRes.plot,qt=="95%"),aes(x=N.Detected,y=qt.HQ.Chronic, label = label.HQ.Chronic))+
  geom_point(aes(colour = CAS.Type))+
  geom_text(angle = 45, hjust = 0, nudge_x = 0.05, size = 3) +
  # geom_text() +
  # geom_hline(yintercept = c(0.1, 0.01), linetype = c(2,3)) +
  geom_hline(yintercept = c(1, 0.1, 0.001, 0.00001), linetype = c(1,2,2,2)) +
  scale_x_log10()+
  # scale_y_log10()+
  theme_classic()+
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"), plot.title = element_text(size = 10), strip.text.x =element_text(size = 14), rect = element_blank(), panel.border=element_rect(size=2,fill=NA, colour='black'),
        axis.text.y = element_text(size = 9), axis.text.x = element_text(size = 14), axis.title.x=element_text(size=12), axis.title.y=element_text(size=12), legend.text=element_text(size=14), legend.title=element_text(size=14),
        legend.key.size=unit(1, "cm"), legend.position="right", legend.box.background = element_rect(size=1,fill=NA))+
  labs(title= paste(paste0("HQ.Chronic.95P  ","N.CAS = ",nrow(SumRes)), dataset.name,Exclusion.CAS, sep = ", "))
Scatter.Plots[[2]] = S.Plot.2
# S.Plot.2
# ggsave(paste("Scatter.CAS.HQ.Chronic.95P-N.Detected", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

S.Plot.3<-ggplot(subset(SumRes.plot,qt=="95%"),aes(x=N.Measured,y=qt.HQ.Acute, label = label.HQ.Acute))+
  geom_point(aes(colour = CAS.Type))+
  geom_text(angle = 45, hjust = 0, nudge_x = 0.05, size = 3) +
  # geom_text() +
  # geom_hline(yintercept = c(0.1, 0.01), linetype = c(2,3)) +
  geom_hline(yintercept = c(1, 0.1, 0.001, 0.00001), linetype = c(1,2,2,2)) +
  scale_x_log10()+
  # scale_y_log10()+
  theme_classic()+
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"), plot.title = element_text(size = 10), strip.text.x =element_text(size = 14), rect = element_blank(), panel.border=element_rect(size=2,fill=NA, colour='black'),
        axis.text.y = element_text(size = 9), axis.text.x = element_text(size = 14), axis.title.x=element_text(size=12), axis.title.y=element_text(size=12), legend.text=element_text(size=14), legend.title=element_text(size=14),
        legend.key.size=unit(1, "cm"), legend.position="right", legend.box.background = element_rect(size=1,fill=NA))+
  labs(x="N.Measured",y="HQ.Acute.95P", title= paste(paste0("HQ.Acute.95P  ","N.CAS = ",nrow(SumRes)), dataset.name,Exclusion.CAS, sep = ", "))
Scatter.Plots[[3]] = S.Plot.3
# S.Plot.3
# ggsave(paste("Scatter.CAS.HQ.Acute.95P-N.Measured", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

S.Plot.4<-ggplot(subset(SumRes.plot,qt=="95%"),aes(x=N.Detected,y=qt.HQ.Acute, label = label.HQ.Acute))+
  geom_point(aes(colour = CAS.Type))+
  geom_text(angle = 45, hjust = 0, nudge_x = 0.05, size = 3) +
  # geom_text() +
  # geom_hline(yintercept = c(0.1, 0.01), linetype = c(2,3)) +
  geom_hline(yintercept = c(1, 0.1, 0.001, 0.00001), linetype = c(1,2,2,2)) +
  scale_x_log10()+
  # scale_y_log10()+
  theme_classic()+
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"), plot.title = element_text(size = 10), strip.text.x =element_text(size = 14), rect = element_blank(), panel.border=element_rect(size=2,fill=NA, colour='black'),
        axis.text.y = element_text(size = 9), axis.text.x = element_text(size = 14), axis.title.x=element_text(size=12), axis.title.y=element_text(size=12), legend.text=element_text(size=14), legend.title=element_text(size=14),
        legend.key.size=unit(1, "cm"), legend.position="right", legend.box.background = element_rect(size=1,fill=NA))+
  labs(title= paste(paste0("HQ.Acute.95P  ","N.CAS = ",nrow(SumRes)), dataset.name,Exclusion.CAS, sep = ", "))
Scatter.Plots[[4]] = S.Plot.4
# S.Plot.4
# ggsave(paste("Scatter.CAS.HQ.Acute.95P-N.Detected", dataset.name,Exclusion.CAS,"pdf", sep = "."),width=6, height = 5, units = "in")

#Combine
# Arrange the plots using patchworks syntax
Scatter.Plots[[2]] + Scatter.Plots[[1]] + Scatter.Plots[[4]] + Scatter.Plots[[3]] +
  plot_layout(guides = 'collect') +
  plot_layout(ncol = 2) +
  plot_annotation(tag_levels = 'A')
```


## Fraction of HI and Risk Accounted by 1,2,3 rest of components.

- Focus only on stations with HI > 1


```{r eval=F}
Data.AggBySiteID.5.CHR = Data.AggBySiteID.4[Data.AggBySiteID.4$HQ_Sum_Chronic> 1,]
Data.AggBySiteID.5.CHR = droplevels(Data.AggBySiteID.5.CHR)
## loop over station ID.Year combinations
  # order by HI
  # restrict to 3 rows
  # sum HI, calculate different for "all other components
  # label 1, 2, 3, Other
  # retain CAS
# rbind with previous stations
Data.drivers = NULL
for ( i in 1:length(unique(Data.AggBySiteID.5.CHR$Site.Year))){
  Site.Year_i = unique(Data.AggBySiteID.5.CHR$Site.Year)[i]
  Data_i = Data.AggBySiteID.5.CHR[Data.AggBySiteID.5.CHR$Site.Year==Site.Year_i,]
  Data_i = Data_i[order(Data_i$HQ.Chronic, decreasing = TRUE),]
  nrow_i = nrow(Data_i)
  if (nrow_i>3) {Data_i = Data_i[c(1:3),c(1:4,5,6,8,9)]} else{Data_i = Data_i[c(1:nrow_i),c(1:4,5,6,8,9)]}
  if (nrow_i==1) {Data_i$Component = 1}
  if (nrow_i==2) {Data_i$Component = c(1,2)}
  if (nrow_i==3) {Data_i$Component = c(1,2,3)}
  if (nrow_i>3) {Data_i = Data_i[c(1:3,3),]; Data_i$Component = c(1,2,3,4);
                                 Data_i[4,5] = Data_i$HQ_Sum_Chronic[1] - (sum(Data_i$HQ.Chronic)-Data_i$HQ.Chronic[4]);
                                 Data_i[4,3] = NA}
  Data.drivers = rbind(Data.drivers,Data_i)
}

# Arrange factor variables
Data.drivers$Component = as.character(Data.drivers$Component)
Data.drivers$Component[Data.drivers$Component=="4"] = "All.Other"
Data.drivers$Component = factor(Data.drivers$Component, levels = c("1", "2","3","All.Other"))
Data.drivers$Perc.Risk = 100*(Data.drivers$HQ.Chronic/Data.drivers$HI.HC05.Chronic.NOEC)

Perc.Contrib.Top3.HI.1 = t(data.frame(sapply(split(Data.drivers$Perc.Risk,Data.drivers$Component), summary, simplify = T)))

# CAS drivers order by feq
CAS.Drivers = as.data.frame(table(Data.drivers$CAS))
CAS.Drivers$N.Site.Year = length(unique(Data.AggBySiteID.5.CHR$Site.Year))
CAS.Drivers$N.Sites = length(unique(Data.AggBySiteID.5.CHR$monitoringSiteIdentifier))
CAS.Drivers$Freq.Perc = round(100* (CAS.Drivers$Freq/CAS.Drivers$N.Site.Year),1)
names(CAS.Drivers)[1] = "CAS"
CAS.Drivers = merge(CAS.Drivers, Data.SSD[,c(1,2,3)], by.x = "CAS", by.y = "CAS.", all.x = T)
CAS.Drivers = CAS.Drivers[order(CAS.Drivers$Freq, decreasing = T),]
head(CAS.Drivers)

```


### Plot Barchart of the HI and Contributions of the top 3 componnents.

```{r}

```

### Boxplot contributions by components

```{r}

```

